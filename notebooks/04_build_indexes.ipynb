{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EraEx: Build FAISS + BM25 Indexes (Colab)\n",
                "\n",
                "This notebook builds:\n",
                "1. FAISS IVF+PQ indexes for dense retrieval\n",
                "2. BM25 index for sparse retrieval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import numpy as np\n",
                "import polars as pl\n",
                "import faiss\n",
                "import pickle\n",
                "\n",
                "PROJECT_DIR = Path('/content/drive/MyDrive/EraEx')\n",
                "EMBEDDINGS_DIR = PROJECT_DIR / 'data' / 'embeddings'\n",
                "READY_DIR = PROJECT_DIR / 'data' / 'processed' / 'music_ready'\n",
                "INDEXES_DIR = PROJECT_DIR / 'data' / 'indexes'\n",
                "\n",
                "INDEXES_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "YEAR_RANGE = range(2012, 2019)\n",
                "\n",
                "N_LIST = 256\n",
                "M_PQ = 48\n",
                "N_BITS = 8"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Build FAISS Indexes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_faiss_index(year):\n",
                "    emb_path = EMBEDDINGS_DIR / f'embeddings_{year}.npy'\n",
                "    idx_path = INDEXES_DIR / f'faiss_{year}.index'\n",
                "    \n",
                "    if not emb_path.exists():\n",
                "        print(f'{year}: No embeddings')\n",
                "        return\n",
                "    \n",
                "    if idx_path.exists():\n",
                "        print(f'{year}: Index exists, skipping')\n",
                "        return\n",
                "    \n",
                "    print(f'\\nBuilding FAISS index for {year}...')\n",
                "    embeddings = np.load(emb_path).astype(np.float32)\n",
                "    n_vectors, dim = embeddings.shape\n",
                "    print(f'  Vectors: {n_vectors:,} x {dim}')\n",
                "    \n",
                "    n_train = min(n_vectors, 100000)\n",
                "    train_idx = np.random.choice(n_vectors, n_train, replace=False)\n",
                "    train_data = embeddings[train_idx]\n",
                "    \n",
                "    n_list_actual = min(N_LIST, n_vectors // 50)\n",
                "    m_pq_actual = min(M_PQ, dim)\n",
                "    \n",
                "    quantizer = faiss.IndexFlatIP(dim)\n",
                "    index = faiss.IndexIVFPQ(quantizer, dim, n_list_actual, m_pq_actual, N_BITS)\n",
                "    \n",
                "    print(f'  Training (n_list={n_list_actual}, m_pq={m_pq_actual})...')\n",
                "    index.train(train_data)\n",
                "    \n",
                "    print('  Adding vectors...')\n",
                "    index.add(embeddings)\n",
                "    \n",
                "    faiss.write_index(index, str(idx_path))\n",
                "    print(f'  Saved: {idx_path}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for year in YEAR_RANGE:\n",
                "    build_faiss_index(year)\n",
                "\n",
                "print('\\nFAISS indexes complete!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Build BM25 Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import bm25s\n",
                "\n",
                "def build_bm25_index():\n",
                "    bm25_path = INDEXES_DIR / 'bm25_index.pkl'\n",
                "    \n",
                "    if bm25_path.exists():\n",
                "        print('BM25 index exists, skipping')\n",
                "        return\n",
                "    \n",
                "    print('Loading all documents for BM25...')\n",
                "    all_docs = []\n",
                "    all_ids = []\n",
                "    \n",
                "    for year in YEAR_RANGE:\n",
                "        data_path = READY_DIR / f'year={year}' / 'data.parquet'\n",
                "        if not data_path.exists():\n",
                "            continue\n",
                "        \n",
                "        df = pl.read_parquet(data_path)\n",
                "        texts = df['doc_text_music'].to_list()\n",
                "        ids = df['track_id'].to_list()\n",
                "        \n",
                "        all_docs.extend([t if t else '' for t in texts])\n",
                "        all_ids.extend([str(i) for i in ids])\n",
                "        \n",
                "        print(f'  {year}: {len(texts):,} docs')\n",
                "    \n",
                "    print(f'\\nTotal documents: {len(all_docs):,}')\n",
                "    \n",
                "    print('Building BM25 index...')\n",
                "    corpus_tokens = bm25s.tokenize(all_docs)\n",
                "    \n",
                "    bm25 = bm25s.BM25()\n",
                "    bm25.index(corpus_tokens)\n",
                "    \n",
                "    index_data = {\n",
                "        'bm25': bm25,\n",
                "        'doc_ids': all_ids,\n",
                "        'corpus_tokens': corpus_tokens,\n",
                "    }\n",
                "    \n",
                "    with open(bm25_path, 'wb') as f:\n",
                "        pickle.dump(index_data, f)\n",
                "    \n",
                "    print(f'Saved: {bm25_path}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "build_bm25_index()\n",
                "\n",
                "print('\\n' + '=' * 50)\n",
                "print('ALL INDEXES COMPLETE')\n",
                "print('=' * 50)\n",
                "\n",
                "for f in sorted(INDEXES_DIR.glob('*')):\n",
                "    size_mb = f.stat().st_size / 1e6\n",
                "    print(f'{f.name}: {size_mb:.1f} MB')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Test Search"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "test_query = 'i miss my ex'\n",
                "print(f'Test query: \"{test_query}\"')\n",
                "\n",
                "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "q_emb = model.encode(test_query, normalize_embeddings=True).astype(np.float32).reshape(1, -1)\n",
                "\n",
                "year = 2015\n",
                "idx_path = INDEXES_DIR / f'faiss_{year}.index'\n",
                "if idx_path.exists():\n",
                "    index = faiss.read_index(str(idx_path))\n",
                "    index.nprobe = 10\n",
                "    scores, indices = index.search(q_emb, 5)\n",
                "    \n",
                "    ids_df = pl.read_parquet(EMBEDDINGS_DIR / f'ids_{year}.parquet')\n",
                "    track_ids = ids_df['track_id'].to_list()\n",
                "    \n",
                "    print(f'\\nTop 5 from {year}:')\n",
                "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
                "        print(f'  {i+1}. Score: {score:.4f}, ID: {track_ids[idx]}')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
