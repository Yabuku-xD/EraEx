{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EraEx: Build FAISS + BM25 Indexes\n",
        "\n",
        "This notebook builds:\n",
        "1. FAISS IVF+PQ indexes for dense retrieval\n",
        "2. BM25 index for sparse retrieval\n",
        "\n",
        "**Supports**: Google Colab, Vast.ai, Local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    PROJECT_DIR = Path('/content/drive/MyDrive/EraEx')\n",
        "except ImportError:\n",
        "    PROJECT_DIR = Path(os.environ.get('ERAEX_DIR', ''))\n",
        "    if not PROJECT_DIR.exists():\n",
        "        if Path.cwd().name == 'notebooks':\n",
        "            PROJECT_DIR = Path.cwd().parent\n",
        "        else:\n",
        "            PROJECT_DIR = Path.cwd()\n",
        "\n",
        "print(f\"Project: {PROJECT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q -r \"{PROJECT_DIR / 'requirements.txt'}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import polars as pl\n",
        "import faiss\n",
        "import pickle\n",
        "import gc\n",
        "\n",
        "EMBEDDINGS_DIR = PROJECT_DIR / 'data' / 'embeddings'\n",
        "READY_DIR = PROJECT_DIR / 'data' / 'processed' / 'music_ready'\n",
        "INDEXES_DIR = PROJECT_DIR / 'data' / 'indexes'\n",
        "\n",
        "INDEXES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "YEAR_RANGE = range(2012, 2019)\n",
        "\n",
        "N_LIST = 256\n",
        "M_PQ = 48\n",
        "N_BITS = 8\n",
        "ADD_CHUNK = 500_000\n",
        "\n",
        "print(f\"Embeddings: {EMBEDDINGS_DIR}\")\n",
        "print(f\"Indexes: {INDEXES_DIR}\\n\")\n",
        "\n",
        "for y in YEAR_RANGE:\n",
        "    p = EMBEDDINGS_DIR / f'embeddings_{y}.npy'\n",
        "    if p.exists():\n",
        "        emb = np.load(p, mmap_mode='r')\n",
        "        print(f\"  {y}: {emb.shape}\")\n",
        "    else:\n",
        "        print(f\"  {y}: NOT FOUND\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Build FAISS Indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_faiss_index(year):\n",
        "    emb_path = EMBEDDINGS_DIR / f'embeddings_{year}.npy'\n",
        "    idx_path = INDEXES_DIR / f'faiss_{year}.index'\n",
        "\n",
        "    if not emb_path.exists():\n",
        "        print(f'{year}: No embeddings')\n",
        "        return\n",
        "\n",
        "    if idx_path.exists():\n",
        "        print(f'{year}: Index exists, skipping')\n",
        "        return\n",
        "\n",
        "    print(f'\\nBuilding FAISS index for {year}...')\n",
        "    emb_mmap = np.load(emb_path, mmap_mode='r')\n",
        "    n_vectors, dim = emb_mmap.shape\n",
        "    print(f'  Vectors: {n_vectors:,} x {dim}')\n",
        "\n",
        "    n_train = min(n_vectors, 100_000)\n",
        "    train_idx = np.random.choice(n_vectors, n_train, replace=False)\n",
        "    train_data = emb_mmap[train_idx].astype(np.float32)\n",
        "\n",
        "    n_list_actual = min(N_LIST, n_vectors // 50)\n",
        "    m_pq_actual = min(M_PQ, dim)\n",
        "\n",
        "    quantizer = faiss.IndexFlatIP(dim)\n",
        "    index = faiss.IndexIVFPQ(quantizer, dim, n_list_actual, m_pq_actual, N_BITS)\n",
        "\n",
        "    print(f'  Training (n_list={n_list_actual}, m_pq={m_pq_actual})...')\n",
        "    index.train(train_data)\n",
        "    del train_data\n",
        "    gc.collect()\n",
        "\n",
        "    print(f'  Adding vectors in chunks of {ADD_CHUNK:,}...')\n",
        "    for start in range(0, n_vectors, ADD_CHUNK):\n",
        "        end = min(start + ADD_CHUNK, n_vectors)\n",
        "        chunk = emb_mmap[start:end].astype(np.float32)\n",
        "        index.add(chunk)\n",
        "        del chunk\n",
        "        gc.collect()\n",
        "        print(f'    {end:,} / {n_vectors:,}')\n",
        "\n",
        "    del emb_mmap\n",
        "    gc.collect()\n",
        "\n",
        "    faiss.write_index(index, str(idx_path))\n",
        "    print(f'  Saved: {idx_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for year in YEAR_RANGE:\n",
        "    build_faiss_index(year)\n",
        "\n",
        "print('\\nFAISS indexes complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Build BM25 Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import bm25s\n",
        "\n",
        "def build_bm25_index():\n",
        "    bm25_path = INDEXES_DIR / 'bm25_index.pkl'\n",
        "    \n",
        "    if bm25_path.exists():\n",
        "        print('BM25 index exists, skipping')\n",
        "        return\n",
        "    \n",
        "    print('Loading all documents for BM25...')\n",
        "    all_docs = []\n",
        "    all_ids = []\n",
        "    \n",
        "    for year in YEAR_RANGE:\n",
        "        data_path = READY_DIR / f'year={year}' / 'data.parquet'\n",
        "        if not data_path.exists():\n",
        "            continue\n",
        "        \n",
        "        df = pl.read_parquet(data_path)\n",
        "        texts = df['doc_text_music'].to_list()\n",
        "        id_col = 'track_id' if 'track_id' in df.columns else 'permalink_url'\n",
        "        ids = df[id_col].to_list()\n",
        "        \n",
        "        all_docs.extend([t if t else '' for t in texts])\n",
        "        all_ids.extend([str(i) for i in ids])\n",
        "        \n",
        "        print(f'  {year}: {len(texts):,} docs')\n",
        "    \n",
        "    print(f'\\nTotal documents: {len(all_docs):,}')\n",
        "    \n",
        "    print('Building BM25 index...')\n",
        "    corpus_tokens = bm25s.tokenize(all_docs)\n",
        "    \n",
        "    bm25 = bm25s.BM25()\n",
        "    bm25.index(corpus_tokens)\n",
        "    \n",
        "    index_data = {\n",
        "        'bm25': bm25,\n",
        "        'doc_ids': all_ids,\n",
        "        'corpus_tokens': corpus_tokens,\n",
        "    }\n",
        "    \n",
        "    with open(bm25_path, 'wb') as f:\n",
        "        pickle.dump(index_data, f)\n",
        "    \n",
        "    print(f'Saved: {bm25_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "build_bm25_index()\n",
        "\n",
        "print('\\n' + '=' * 50)\n",
        "print('ALL INDEXES COMPLETE')\n",
        "print('=' * 50)\n",
        "\n",
        "for f in sorted(INDEXES_DIR.glob('*')):\n",
        "    size_mb = f.stat().st_size / 1e6\n",
        "    print(f'{f.name}: {size_mb:.1f} MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "test_query = 'i miss my ex'\n",
        "print(f'Test query: \"{test_query}\"')\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "q_emb = model.encode(test_query, normalize_embeddings=True).astype(np.float32).reshape(1, -1)\n",
        "\n",
        "year = 2015\n",
        "idx_path = INDEXES_DIR / f'faiss_{year}.index'\n",
        "if idx_path.exists():\n",
        "    index = faiss.read_index(str(idx_path))\n",
        "    index.nprobe = 10\n",
        "    scores, indices = index.search(q_emb, 5)\n",
        "    \n",
        "    ids_df = pl.read_parquet(EMBEDDINGS_DIR / f'ids_{year}.parquet')\n",
        "    track_ids = ids_df['track_id'].to_list()\n",
        "    \n",
        "    print(f'\\nTop 5 from {year}:')\n",
        "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
        "        print(f'  {i+1}. Score: {score:.4f}, ID: {track_ids[idx]}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
