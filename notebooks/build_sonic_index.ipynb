{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sonic Index Builder (Vast.ai Ready)\n",
                "\n",
                "This notebook builds the audio and semantic index for the EraEx recommendation system.\n",
                "It is configured to run in a cloud environment (like Vast.ai) or locally.\n",
                "\n",
                "## üöÄ SPEED UPDATE: Parallel Processing\n",
                "This version uses `ThreadPoolExecutor` to download and process multiple tracks simultaneously.\n",
                "\n",
                "## Nostalgia Enforcement üï∞Ô∏è\n",
                "This version specifically crawls playlists from **2012-2018** and strictly enforces date checks.\n",
                "\n",
                "## Instructions for Vast.ai Users:\n",
                "1. Upload the entire `EraEx` folder (or at least `notebooks/`, `src/`, and `requirements.txt`).\n",
                "2. Run the Setup & Imports cells below.\n",
                "3. Set `CANDIDATE_LIMIT` to 100,000 to maximize yield after filtering.\n",
                "4. When done, download `data/indices/sonic_index.pkl`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -r ../requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import warnings\n",
                "\n",
                "# Suppress warnings (librosa/ffmpeg/tensorflow)\n",
                "warnings.filterwarnings('ignore')\n",
                "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
                "\n",
                "# 2. Path Setup\n",
                "# Add project root to path so we can import 'src'\n",
                "# Assuming this notebook is in <root>/notebooks/\n",
                "notebook_dir = os.getcwd()\n",
                "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
                "\n",
                "if project_root not in sys.path:\n",
                "    sys.path.append(project_root)\n",
                "    \n",
                "print(f\"Project Root: {project_root}\")\n",
                "print(f\"System Path Updated.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import pickle\n",
                "import numpy as np\n",
                "import time\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "try:\n",
                "    from src.audio.processor import AudioProcessor\n",
                "    from src.audio.semantic import SemanticEncoder\n",
                "    from src.ranking import nostalgia  # Import Nostalgia for date checking\n",
                "    print(\"‚úÖ Modules imported successfully.\")\n",
                "except ImportError as e:\n",
                "    print(f\"‚ùå Import Error: {e}\")\n",
                "    print(\"Make sure you uploaded the 'src' folder along with this notebook!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration\n",
                "\n",
                "Set the limit for how many candidates to process. We crawl playlists from 2012-2018."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CANDIDATE_LIMIT = 100000  # Increased heavily as filter rejects ~98%\n",
                "YEARS = [2012, 2013, 2014, 2015, 2016, 2017, 2018]\n",
                "OUTPUT_DIR = os.path.join(project_root, 'data', 'indices')\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "OUTPUT_FILE = os.path.join(OUTPUT_DIR, 'sonic_index.pkl')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build Logic (Playlist Crawling)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def fetch_tracks_from_year(year, limit=CANDIDATE_LIMIT):\n",
                "    \"\"\"Fetches tracks from MULTIPLE top playlists for a specific year, covering ALL genres.\"\"\"\n",
                "    all_tracks = []\n",
                "    seen_ids = set()\n",
                "    \n",
                "    # 1. Get Genres Dynamically\n",
                "    try:\n",
                "        from src.data.deezer import DeezerCollector\n",
                "        collector = DeezerCollector()\n",
                "        raw_genres = collector.get_genres()\n",
                "        # The get_genres returns a list, clean usage here\n",
                "        if isinstance(raw_genres, list):\n",
                "            genres = raw_genres\n",
                "        else:\n",
                "            genres = ['pop']\n",
                "        print(f\"Fetched {len(genres)} genres from Deezer.\")\n",
                "    except Exception as e:\n",
                "        print(f\"Could not fetch genres: {e}. Using defaults.\")\n",
                "        genres = ['pop', 'rock', 'rap', 'hip hop', 'jazz', 'metal', 'alternative', 'dance', 'electronic', 'r&b', 'soul', 'reggae', 'indie', 'folk', 'country', 'latin']\n",
                "        \n",
                "    # 2. Build Query List\n",
                "    # Core queries\n",
                "    search_queries = [f\"Top {year}\", f\"Best of {year}\", f\"{year} Hits\"]\n",
                "    # Genre-specific queries\n",
                "    for g in genres:\n",
                "        if len(g) > 2: # Skip tiny genre names\n",
                "             search_queries.append(f\"{year} {g}\")\n",
                "        \n",
                "    # Shuffle queries to ensure variety if we hit limit early\n",
                "    np.random.shuffle(search_queries)\n",
                "    \n",
                "    print(f\"Generated {len(search_queries)} search queries for {year} (covering all genres).\")\n",
                "    \n",
                "    for query in search_queries:\n",
                "        if len(all_tracks) >= limit: break\n",
                "        \n",
                "        # print(f\"Searching playlists for: {query}...\") # Verbose\n",
                "        search_url = \"https://api.deezer.com/search/playlist\"\n",
                "        try:\n",
                "            # Get top 30 playlists for this specific query (Increased to scale!)\n",
                "            r = requests.get(search_url, params={'q': query, 'limit': 30})\n",
                "            playlists = r.json().get('data', [])\n",
                "            \n",
                "            if not playlists: continue\n",
                "            \n",
                "            for pl in playlists:\n",
                "                if len(all_tracks) >= limit: break\n",
                "                if pl['id'] in seen_ids: continue # Avoid re-crawling same playlist\n",
                "                \n",
                "                print(f\"  -> Crawling: {pl['title']} ({pl['nb_tracks']} tracks)\")\n",
                "                seen_ids.add(pl['id']) \n",
                "                \n",
                "                # 3. Get tracks\n",
                "                tracks_url = f\"https://api.deezer.com/playlist/{pl['id']}/tracks\"\n",
                "                r_t = requests.get(tracks_url, params={'limit': 100})\n",
                "                pl_tracks = r_t.json().get('data', [])\n",
                "                \n",
                "                for t in pl_tracks:\n",
                "                    if t['id'] not in seen_ids:\n",
                "                        pass\n",
                "                \n",
                "                # Fix: separate sets\n",
                "                # Actually, let's keep it simple. Only add track if ID not in all_tracks IDs.\n",
                "                current_track_ids = set(x['id'] for x in all_tracks)\n",
                "                for t in pl_tracks:\n",
                "                     if t['id'] not in current_track_ids and len(all_tracks) < limit:\n",
                "                         all_tracks.append(t)\n",
                "                \n",
                "                time.sleep(0.1)\n",
                "                \n",
                "        except Exception as e:\n",
                "            continue\n",
                "            \n",
                "    return all_tracks[:limit]\n",
                "\n",
                "def get_release_date(track):\n",
                "    \"\"\"Helper to get release date, fetching album if needed.\"\"\"\n",
                "    if 'release_date' in track:\n",
                "        return track['release_date']\n",
                "        \n",
                "    # Optimization: We fetch the album to be strictly accurate.\n",
                "    try:\n",
                "        alb_id = track['album']['id']\n",
                "        r = requests.get(f\"https://api.deezer.com/album/{alb_id}\")\n",
                "        return r.json().get('release_date')\n",
                "    except:\n",
                "        return None\n",
                "\n",
                "def build_index(limit_per_year=CANDIDATE_LIMIT):\n",
                "    print(f\"--- Starting Sonic Index Build (Target: 2012-2018) ---\")\n",
                "    \n",
                "    # 1. Initialize Engines\n",
                "    try:\n",
                "        audio_proc = AudioProcessor()\n",
                "        semantic_enc = SemanticEncoder()\n",
                "        # FIX: Instantiate the class\n",
                "        nostalgia_filter = nostalgia.NostalgiaFilter()\n",
                "    except Exception as e:\n",
                "        print(f\"Init Error: {e}\")\n",
                "        return\n",
                "    \n",
                "    sonic_data = []\n",
                "    seen_ids = set()\n",
                "    \n",
                "    # 2. Iterate Years\n",
                "    for year in YEARS:\n",
                "        tracks = fetch_tracks_from_year(year, limit=limit_per_year)\n",
                "        print(f\"[{year}] Found {len(tracks)} candidates. Processing in PARALLEL...\")\n",
                "        \n",
                "        kept = 0\n",
                "        skipped_era = 0\n",
                "        skipped_no_preview = 0\n",
                "        skipped_error = 0\n",
                "        \n",
                "        # Define the worker function for parallel execution\n",
                "        def process_one_track(track):\n",
                "            # STRICT NOSTALGIA CHECK (Network Call 1)\n",
                "            r_date = get_release_date(track)\n",
                "            if not r_date or not nostalgia_filter.is_in_era(r_date):\n",
                "                return 'skipped_era', None\n",
                "            \n",
                "            try:\n",
                "                # A. Audio Analysis (Network Call 2 + CPU)\n",
                "                preview_url = track.get('preview')\n",
                "                audio_vec = None\n",
                "                \n",
                "                if not preview_url:\n",
                "                     return 'skipped_no_preview', None\n",
                "                     \n",
                "                if preview_url:\n",
                "                    result = audio_proc.analyze_url(preview_url)\n",
                "                    if result:\n",
                "                        audio_vec = result['vector']\n",
                "                \n",
                "                # B. Semantic Analysis\n",
                "                meta_text = f\"{track['title']} by {track['artist']['name']} album {track['album']['title']}\"\n",
                "                semantic_vec = semantic_enc.encode(meta_text)\n",
                "                \n",
                "                # C. Return Result\n",
                "                if audio_vec is not None and semantic_vec is not None:\n",
                "                    return 'kept', {\n",
                "                        'id': track['id'],\n",
                "                        'title': track['title'],\n",
                "                        'artist': track['artist']['name'],\n",
                "                        'year': year,\n",
                "                        'release_date': r_date,\n",
                "                        'audio_vector': audio_vec,\n",
                "                        'semantic_vector': semantic_vec,\n",
                "                        'preview': preview_url\n",
                "                    }\n",
                "                else:\n",
                "                    return 'skipped_error', None\n",
                "                    \n",
                "            except Exception:\n",
                "                return 'skipped_error', None\n",
                "\n",
                "        # PARALLEL EXECUTION\n",
                "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
                "        \n",
                "        # Use 16 threads to saturate connection\n",
                "        with ThreadPoolExecutor(max_workers=16) as executor:\n",
                "            # Filter distinct tracks first\n",
                "            unique_candidates = []\n",
                "            for t in tracks:\n",
                "                if t['id'] not in seen_ids:\n",
                "                     unique_candidates.append(t)\n",
                "                     seen_ids.add(t['id'])\n",
                "            \n",
                "            # Submit all tasks\n",
                "            future_to_track = {executor.submit(process_one_track, t): t for t in unique_candidates}\n",
                "            \n",
                "            for future in tqdm(as_completed(future_to_track), total=len(unique_candidates), desc=f\"Processing {year}\"):\n",
                "                status, data = future.result()\n",
                "                \n",
                "                if status == 'kept':\n",
                "                    sonic_data.append(data)\n",
                "                    kept += 1\n",
                "                elif status == 'skipped_era':\n",
                "                    skipped_era += 1\n",
                "                elif status == 'skipped_no_preview':\n",
                "                    skipped_no_preview += 1\n",
                "                else:\n",
                "                    skipped_error += 1\n",
                "            \n",
                "        print(f\"[{year} Summary] Kept: {kept} | Out of Era: {skipped_era} | No Preview: {skipped_no_preview} | Errors: {skipped_error}\")\n",
                "\n",
                "    # 3. Save Index\n",
                "    with open(OUTPUT_FILE, 'wb') as f:\n",
                "        pickle.dump(sonic_data, f)\n",
                "        \n",
                "    print(f\"\\n--- Build Complete ---\")\n",
                "    print(f\"Saved {len(sonic_data)} tracks to {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the build\n",
                "build_index(limit_per_year=CANDIDATE_LIMIT)\n",
                "\n",
                "# Optional: rclone command reminder\n",
                "# !rclone copy ./data/indices/sonic_index.pkl remote:era_ex_backup/"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}