{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EraEx: NDJSON â†’ CSV Conversion (Colab)\n",
                "\n",
                "This notebook reads `.ndjson.zst` files and outputs a **CSV file** for combining with your existing dataset.\n",
                "\n",
                "**Output**: `data/raw/ndjson_converted.csv`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import json\n",
                "import zstandard as zstd\n",
                "import polars as pl\n",
                "from datetime import date\n",
                "from typing import Iterator, Dict, Any, Optional\n",
                "\n",
                "PROJECT_DIR = Path('/content/drive/MyDrive/EraEx')\n",
                "RAW_DIR = PROJECT_DIR / 'data' / 'raw'\n",
                "\n",
                "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "YEAR_RANGE = range(2012, 2019)\n",
                "\n",
                "print(f'Raw data: {RAW_DIR}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ndjson_files = list(RAW_DIR.glob('*.ndjson.zst')) + list(RAW_DIR.glob('*.ndjson'))\n",
                "print(f'Found {len(ndjson_files)} NDJSON file(s):')\n",
                "for f in ndjson_files:\n",
                "    print(f'  - {f.name} ({f.stat().st_size / 1e9:.2f} GB)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "NDJSON_FIELD_MAPPING = {\n",
                "    'id': 'track_id',\n",
                "    'title': 'title',\n",
                "    'user.username': 'artist',\n",
                "    'genre': 'genre',\n",
                "    'tag_list': 'tags',\n",
                "    'description': 'description',\n",
                "    'playback_count': 'playback_count',\n",
                "    'permalink_url': 'permalink_url',\n",
                "    'created_at': 'created_at',\n",
                "}\n",
                "\n",
                "def extract_nested_field(obj: dict, path: str) -> Any:\n",
                "    parts = path.split('.')\n",
                "    current = obj\n",
                "    for part in parts:\n",
                "        if current is None:\n",
                "            return None\n",
                "        if isinstance(current, dict):\n",
                "            current = current.get(part)\n",
                "        else:\n",
                "            return None\n",
                "    return current\n",
                "\n",
                "def parse_ndjson_line(line: str) -> Optional[Dict[str, Any]]:\n",
                "    try:\n",
                "        obj = json.loads(line)\n",
                "    except json.JSONDecodeError:\n",
                "        return None\n",
                "    \n",
                "    mapped = {}\n",
                "    for source_field, target_field in NDJSON_FIELD_MAPPING.items():\n",
                "        value = extract_nested_field(obj, source_field)\n",
                "        mapped[target_field] = value\n",
                "    \n",
                "    if mapped.get('created_at'):\n",
                "        try:\n",
                "            mapped['year'] = int(mapped['created_at'][:4])\n",
                "        except (ValueError, TypeError):\n",
                "            mapped['year'] = None\n",
                "    else:\n",
                "        mapped['year'] = None\n",
                "    \n",
                "    mapped['track_id'] = str(mapped.get('track_id', '')) if mapped.get('track_id') else None\n",
                "    \n",
                "    return mapped"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def stream_ndjson_zst(file_path: Path, chunk_size: int = 10000) -> Iterator[list]:\n",
                "    dctx = zstd.ZstdDecompressor()\n",
                "    \n",
                "    with open(file_path, 'rb') as fh:\n",
                "        with dctx.stream_reader(fh) as reader:\n",
                "            buffer = b''\n",
                "            chunk = []\n",
                "            \n",
                "            while True:\n",
                "                data = reader.read(1024 * 1024)\n",
                "                if not data:\n",
                "                    break\n",
                "                \n",
                "                buffer += data\n",
                "                lines = buffer.split(b'\\n')\n",
                "                buffer = lines[-1]\n",
                "                \n",
                "                for line in lines[:-1]:\n",
                "                    if not line.strip():\n",
                "                        continue\n",
                "                    \n",
                "                    parsed = parse_ndjson_line(line.decode('utf-8', errors='ignore'))\n",
                "                    if parsed and parsed.get('year') in YEAR_RANGE:\n",
                "                        chunk.append(parsed)\n",
                "                    \n",
                "                    if len(chunk) >= chunk_size:\n",
                "                        yield chunk\n",
                "                        chunk = []\n",
                "            \n",
                "            if buffer.strip():\n",
                "                parsed = parse_ndjson_line(buffer.decode('utf-8', errors='ignore'))\n",
                "                if parsed and parsed.get('year') in YEAR_RANGE:\n",
                "                    chunk.append(parsed)\n",
                "            \n",
                "            if chunk:\n",
                "                yield chunk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output_csv = RAW_DIR / 'ndjson_converted.csv'\n",
                "first_chunk = True\n",
                "total_rows = 0\n",
                "\n",
                "for ndjson_file in ndjson_files:\n",
                "    print(f'Processing: {ndjson_file.name}')\n",
                "    \n",
                "    for chunk in stream_ndjson_zst(ndjson_file, chunk_size=50000):\n",
                "        df = pl.DataFrame(chunk)\n",
                "        \n",
                "        if first_chunk:\n",
                "            df.write_csv(output_csv)\n",
                "            first_chunk = False\n",
                "        else:\n",
                "            with open(output_csv, 'ab') as f:\n",
                "                df.write_csv(f, include_header=False)\n",
                "        \n",
                "        total_rows += len(chunk)\n",
                "        if total_rows % 500000 == 0:\n",
                "            print(f'  Processed: {total_rows:,} rows')\n",
                "\n",
                "print(f'\\nDone! Total rows: {total_rows:,}')\n",
                "print(f'Output: {output_csv}')\n",
                "print(f'Size: {output_csv.stat().st_size / 1e9:.2f} GB')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
