{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EraEx: ColBERT Embedding (Colab GPU)\n",
                "\n",
                "This notebook generates ColBERT embeddings for music tracks.\n",
                "\n",
                "**Model**: colbert-ir/colbertv2.0\n",
                "**Fallback**: sentence-transformers/all-MiniLM-L6-v2\n",
                "\n",
                "**Requirements**: GPU runtime (T4 or better)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "except ImportError:\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "print(f'CUDA available: {torch.cuda.is_available()}')\n",
                "if torch.cuda.is_available():\n",
                "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import numpy as np\n",
                "import polars as pl\n",
                "import os\n",
                "from tqdm import tqdm\n",
                "\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    PROJECT_DIR = Path('/content/drive/MyDrive/EraEx')\n",
                "except ImportError:\n",
                "    PROJECT_DIR = Path.cwd().parent\n",
                "\n",
                "READY_DIR = PROJECT_DIR / 'data' / 'processed' / 'music_ready'\n",
                "EMBEDDINGS_DIR = PROJECT_DIR / 'data' / 'embeddings'\n",
                "\n",
                "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "YEAR_RANGE = range(2012, 2019)\n",
                "USE_COLBERT = True  # Set False to use SBERT fallback\n",
                "print(f'Project Dir: {PROJECT_DIR}')\n",
                "print(f'Looking for data in: {READY_DIR}')  # DEBUG PRINT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "if USE_COLBERT:\n",
                "    try:\n",
                "        from transformers import AutoTokenizer, AutoModel, logging\n",
                "        \n",
                "        # Suppress HF warnings\n",
                "        logging.set_verbosity_error()\n",
                "        import warnings\n",
                "        warnings.filterwarnings('ignore', category=UserWarning)\n",
                "\n",
                "        MODEL_NAME = 'colbert-ir/colbertv2.0'\n",
                "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "        \n",
                "        print(f'Loading {MODEL_NAME}...')\n",
                "        print(\"(Note: Ignore 'UNEXPECTED linear.weight' warnings - normal for Dense mode)\")\n",
                "        \n",
                "        model = AutoModel.from_pretrained(MODEL_NAME)\n",
                "        model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "        model.eval()\n",
                "        EMBEDDER_TYPE = 'colbert'\n",
                "        print(f'âœ“ Model Loaded Successfully')\n",
                "    except Exception as e:\n",
                "        print(f'ColBERT failed: {e}')\n",
                "        print('Falling back to SBERT')\n",
                "        USE_COLBERT = False\n",
                "\n",
                "if not USE_COLBERT:\n",
                "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "    EMBEDDER_TYPE = 'sbert'\n",
                "    print('Using SBERT: all-MiniLM-L6-v2')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def embed_sbert(texts, batch_size=256):\n",
                "    embeddings = model.encode(\n",
                "        texts,\n",
                "        batch_size=batch_size,\n",
                "        show_progress_bar=True,\n",
                "        convert_to_numpy=True,\n",
                "        normalize_embeddings=True\n",
                "    )\n",
                "    return embeddings.astype(np.float16)\n",
                "\n",
                "def embed_colbert(texts, batch_size=32):\n",
                "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "    all_embeddings = []\n",
                "    \n",
                "    for i in tqdm(range(0, len(texts), batch_size)):\n",
                "        batch = texts[i:i + batch_size]\n",
                "        \n",
                "        inputs = tokenizer(\n",
                "            batch,\n",
                "            padding=True,\n",
                "            truncation=True,\n",
                "            max_length=180,\n",
                "            return_tensors='pt'\n",
                "        ).to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = model(**inputs)\n",
                "            token_emb = outputs.last_hidden_state\n",
                "            mask = inputs['attention_mask']\n",
                "            \n",
                "            mask_expanded = mask.unsqueeze(-1).float()\n",
                "            sum_emb = (token_emb * mask_expanded).sum(dim=1)\n",
                "            count = mask_expanded.sum(dim=1)\n",
                "            pooled = sum_emb / count\n",
                "            pooled = pooled / (pooled.norm(dim=1, keepdim=True) + 1e-9)\n",
                "            \n",
                "            all_embeddings.append(pooled.cpu().numpy())\n",
                "    \n",
                "    return np.vstack(all_embeddings).astype(np.float16)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_year(year):\n",
                "    data_path = READY_DIR / f'year={year}' / 'data.parquet'\n",
                "    if not data_path.exists():\n",
                "        print(f'{year}: Not found at {data_path}')\n",
                "        # Helpful debug for file structure mismatch\n",
                "        if year == 2012:\n",
                "            print(\"  > Debug: Contents of parent folder:\")\n",
                "            try:\n",
                "                if READY_DIR.parent.exists():\n",
                "                    print(list(READY_DIR.parent.iterdir()))\n",
                "                else:\n",
                "                    print(f\"    Parent {READY_DIR.parent} does not exist!\")\n",
                "            except Exception as e:\n",
                "                print(f\"    Could not list parent: {e}\")\n",
                "        return\n",
                "    \n",
                "    emb_path = EMBEDDINGS_DIR / f'embeddings_{year}.npy'\n",
                "    ids_path = EMBEDDINGS_DIR / f'ids_{year}.parquet'\n",
                "    \n",
                "    if emb_path.exists() and ids_path.exists():\n",
                "        print(f'{year}: Already exists, skipping')\n",
                "        return\n",
                "    \n",
                "    print(f'\\nProcessing {year}...')\n",
                "    try:\n",
                "        df = pl.read_parquet(data_path)\n",
                "    except Exception as e:\n",
                "        print(f\"Error reading parquet {data_path}: {e}\")\n",
                "        return\n",
                "    \n",
                "    print(f'  Rows: {df.height:,}')\n",
                "    \n",
                "    texts = df['doc_text_music'].to_list()\n",
                "    texts = [t if t else '' for t in texts]\n",
                "    \n",
                "    print(f'  Embedding with {EMBEDDER_TYPE}...')\n",
                "    if EMBEDDER_TYPE == 'colbert':\n",
                "        embeddings = embed_colbert(texts)\n",
                "    else:\n",
                "        embeddings = embed_sbert(texts)\n",
                "    \n",
                "    np.save(emb_path, embeddings)\n",
                "    print(f'  Saved: {emb_path}')\n",
                "    \n",
                "    ids_df = df.select(['track_id'])\n",
                "    ids_df.write_parquet(ids_path)\n",
                "    print(f'  Saved: {ids_path}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for year in YEAR_RANGE:\n",
                "    process_year(year)\n",
                "\n",
                "print('\\n' + '=' * 50)\n",
                "print('EMBEDDING COMPLETE')\n",
                "print('=' * 50)\n",
                "\n",
                "for f in sorted(EMBEDDINGS_DIR.glob('*.npy')):\n",
                "    emb = np.load(f)\n",
                "    print(f'{f.name}: {emb.shape}')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}