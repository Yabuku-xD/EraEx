{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EraEx: Full Production Deployment Pipeline\n",
    "\n",
    "**Use this notebook to Initialize the System from Scratch.**\n",
    "\n",
    "> **WARNING**: This process is computationally expensive. It generates embeddings for 780,000+ songs. Run on a machine with a GPU.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies\n",
    "Ensure `ffmpeg` is installed on your system path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install librosa torch transformers sentence-transformers faiss-cpu python-dotenv tqdm accelerate vllm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "try:\n",
    "    import vllm  # noqa: F401\n",
    "    print(\"vLLM available: True\")\n",
    "except Exception as exc:\n",
    "    print(f\"vLLM available: False ({exc})\")\n",
    "    print(\"Install/upgrade with: %pip install -U vllm, then restart runtime.\")\n",
    "\n",
    "\n",
    "def _resolve_project_root():\n",
    "    env_root = os.getenv(\"ERAEX_PROJECT_ROOT\", \"\").strip()\n",
    "    candidates = []\n",
    "    if env_root:\n",
    "        candidates.append(Path(env_root).expanduser())\n",
    "\n",
    "    cwd = Path.cwd().resolve()\n",
    "    candidates.extend([cwd, cwd.parent])\n",
    "\n",
    "    common = [\n",
    "        Path(\"/content/Team4_CPSC-5830-01-Capstone-Project\"),\n",
    "        Path(\"/content/drive/MyDrive/Team4_CPSC-5830-01-Capstone-Project\"),\n",
    "        Path(\"/content/drive/MyDrive/EraEx\"),\n",
    "    ]\n",
    "    candidates.extend(common)\n",
    "\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for c in candidates:\n",
    "        key = str(c)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        deduped.append(c)\n",
    "\n",
    "    for root in deduped:\n",
    "        if (root / \"src\").exists() and (root / \"config\").exists():\n",
    "            return root\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"Could not locate project root with src/ and config/. \"\n",
    "        \"Set ERAEX_PROJECT_ROOT, e.g. %env ERAEX_PROJECT_ROOT=/content/Team4_CPSC-5830-01-Capstone-Project\"\n",
    "    )\n",
    "\n",
    "\n",
    "PROJECT_ROOT = _resolve_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "load_dotenv(PROJECT_ROOT / \".env\", override=False)\n",
    "hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if hf_token:\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "    os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token\n",
    "print(f\"PROJECT_ROOT={PROJECT_ROOT}\")\n",
    "\n",
    "from src.core.text_embeddings import embedding_handler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# --- Colab/H100 friendly I/O setup ---\n",
    "# Put source CSV on fast local disk (/content) first, then sync outputs back.\n",
    "SOURCE_DATA_PATH = Path(os.getenv(\"ERAEX_SOURCE_CSV\", \"../data/EraEx_Dataset_Rich.csv\")).expanduser()\n",
    "DEFAULT_LOCAL_RUN_DIR = \"/content/eraex_run\" if Path(\"/content\").exists() else \"../data\"\n",
    "LOCAL_RUN_DIR = Path(os.getenv(\"ERAEX_LOCAL_RUN_DIR\", DEFAULT_LOCAL_RUN_DIR)).expanduser()\n",
    "LOCAL_RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOCAL_DATA_PATH = LOCAL_RUN_DIR / \"EraEx_Dataset_Rich.csv\"\n",
    "RICH_CSV_PATH_DEFAULT = LOCAL_RUN_DIR / \"EraEx_Dataset_Rich.csv\"\n",
    "INDEX_DIR = LOCAL_RUN_DIR / \"indexes\"\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional sync controls (for Drive or rclone remote).\n",
    "SYNC_ENABLED = bool(int(os.getenv(\"ERAEX_SYNC_ENABLED\", \"0\")))\n",
    "SYNC_MODE = os.getenv(\"ERAEX_SYNC_MODE\", \"copy\").strip().lower()  # copy | rclone\n",
    "SYNC_TARGET_DIR = os.getenv(\"ERAEX_SYNC_TARGET_DIR\", \"\").strip()   # e.g. /content/drive/MyDrive/eraex\n",
    "RCLONE_REMOTE = os.getenv(\"ERAEX_RCLONE_REMOTE\", \"\").strip()       # e.g. gdrive:eraex\n",
    "\n",
    "\n",
    "def sync_path(local_path: Path, relative_name: str):\n",
    "    local_path = Path(local_path)\n",
    "    if not SYNC_ENABLED:\n",
    "        return\n",
    "    if not local_path.exists():\n",
    "        return\n",
    "\n",
    "    if SYNC_MODE == \"copy\":\n",
    "        if not SYNC_TARGET_DIR:\n",
    "            print(\"[SYNC] skipped: ERAEX_SYNC_TARGET_DIR not set\")\n",
    "            return\n",
    "        target = Path(SYNC_TARGET_DIR) / relative_name\n",
    "        target.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if local_path.is_dir():\n",
    "            if target.exists():\n",
    "                shutil.rmtree(target, ignore_errors=True)\n",
    "            shutil.copytree(local_path, target)\n",
    "        else:\n",
    "            shutil.copy2(local_path, target)\n",
    "        print(f\"[SYNC] copied -> {target}\")\n",
    "        return\n",
    "\n",
    "    if SYNC_MODE == \"rclone\":\n",
    "        if not RCLONE_REMOTE:\n",
    "            print(\"[SYNC] skipped: ERAEX_RCLONE_REMOTE not set\")\n",
    "            return\n",
    "        remote_path = f\"{RCLONE_REMOTE.rstrip('/')}/{relative_name}\"\n",
    "        if local_path.is_dir():\n",
    "            cmd = [\"rclone\", \"sync\", str(local_path), remote_path, \"--progress\"]\n",
    "        else:\n",
    "            cmd = [\"rclone\", \"copyto\", str(local_path), remote_path, \"--progress\"]\n",
    "        try:\n",
    "            subprocess.run(cmd, check=True)\n",
    "            print(f\"[SYNC] rclone -> {remote_path}\")\n",
    "        except Exception as exc:\n",
    "            print(f\"[SYNC] rclone failed: {exc}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[SYNC] skipped: unknown SYNC_MODE={SYNC_MODE}\")\n",
    "\n",
    "\n",
    "if not SOURCE_DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Source CSV not found: {SOURCE_DATA_PATH}\")\n",
    "\n",
    "if SOURCE_DATA_PATH.resolve() != LOCAL_DATA_PATH.resolve():\n",
    "    needs_copy = (not LOCAL_DATA_PATH.exists()) or (SOURCE_DATA_PATH.stat().st_mtime > LOCAL_DATA_PATH.stat().st_mtime)\n",
    "    if needs_copy:\n",
    "        print(f\"Copying source CSV to local fast storage: {SOURCE_DATA_PATH} -> {LOCAL_DATA_PATH}\")\n",
    "        shutil.copy2(SOURCE_DATA_PATH, LOCAL_DATA_PATH)\n",
    "\n",
    "DATA_PATH = str(LOCAL_DATA_PATH)\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "print(f\"Loaded {len(df)} tracks from {DATA_PATH}\")\n",
    "print(f\"LOCAL_RUN_DIR={LOCAL_RUN_DIR}\")\n",
    "print(f\"INDEX_DIR={INDEX_DIR}\")\n",
    "\n",
    "# --- Column mapping audit (ensures all downstream fields are matched) ---\n",
    "COLUMN_ALIASES = {\n",
    "    \"track_id\": [\"Track ID\", \"deezer_id\", \"video_id\"],\n",
    "    \"title\": [\"Track Title\", \"title\"],\n",
    "    \"artist\": [\"Artist Name\", \"artist_name\"],\n",
    "    \"album\": [\"Album Title\", \"album_title\"],\n",
    "    \"release_year\": [\"Release Year\", \"year\"],\n",
    "    \"tags\": [\"Deezer Tags\", \"deezer_tags\"],\n",
    "    \"playcount\": [\"Play Count\", \"deezer_playcount\"],\n",
    "    \"deezer_rank\": [\"Deezer Rank\", \"deezer_rank\", \"rank\"],\n",
    "    \"views\": [\"Views\", \"views\"],\n",
    "    \"cover\": [\"Cover URL\", \"cover_url\"],\n",
    "    \"description\": [\"Description\", \"description\", \"Track Description\", \"YouTube Description\"],\n",
    "    \"instrumental\": [\"Instrumental\", \"instrumental\", \"is_instrumental\"],\n",
    "    \"instrumental_confidence\": [\"Instrumental Confidence\", \"instrumental_confidence\"],\n",
    "}\n",
    "\n",
    "COLUMN_MAP = {}\n",
    "for canonical, aliases in COLUMN_ALIASES.items():\n",
    "    matched = next((col for col in aliases if col in df.columns), None)\n",
    "    COLUMN_MAP[canonical] = matched\n",
    "\n",
    "print(\"\\nColumn mapping summary:\")\n",
    "for canonical, col in COLUMN_MAP.items():\n",
    "    print(f\"- {canonical}: {col}\")\n",
    "\n",
    "critical = [\"track_id\", \"title\", \"artist\", \"tags\", \"deezer_rank\"]\n",
    "missing_critical = [name for name in critical if COLUMN_MAP.get(name) is None]\n",
    "if missing_critical:\n",
    "    raise RuntimeError(f\"Missing critical columns: {missing_critical}\")\n",
    "\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 One-Row LLM Description Preview (Qwen 2.5 3B)\n",
    "Run this to preview one generated song description from your CSV before full enrichment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "PREVIEW_ROW_INDEX = 0\n",
    "MAX_NEW_TOKENS = 120\n",
    "TEMPERATURE = 0.30\n",
    "TOP_P = 0.90\n",
    "\n",
    "\n",
    "def _safe(value, default=\"\"):\n",
    "    if value is None:\n",
    "        return default\n",
    "    text = str(value).strip()\n",
    "    if not text or text.lower() == \"nan\":\n",
    "        return default\n",
    "    return text\n",
    "\n",
    "\n",
    "def _tags_text(row_dict):\n",
    "    raw = row_dict.get(\"Deezer Tags\", row_dict.get(\"deezer_tags\", \"\"))\n",
    "    if isinstance(raw, list):\n",
    "        return \", \".join(str(v).strip() for v in raw if str(v).strip())\n",
    "    raw = _safe(raw, \"\")\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    try:\n",
    "        parsed = json.loads(raw.replace(\"'\", '\"'))\n",
    "        if isinstance(parsed, list):\n",
    "            return \", \".join(str(v).strip() for v in parsed if str(v).strip())\n",
    "    except Exception:\n",
    "        pass\n",
    "    return raw\n",
    "\n",
    "\n",
    "def _extract_first_json(text):\n",
    "    text = str(text or \"\").strip()\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "    match = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(match.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _strip_entities(text, entities):\n",
    "    cleaned = str(text or \"\")\n",
    "    for ent in entities:\n",
    "        ent = str(ent or \"\").strip()\n",
    "        if len(ent) < 4:\n",
    "            continue\n",
    "        cleaned = re.sub(re.escape(ent), \"\", cleaned, flags=re.IGNORECASE)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", cleaned)\n",
    "    cleaned = re.sub(r\"^[,;:.\\- ]+\", \"\", cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "\n",
    "row = df.iloc[int(PREVIEW_ROW_INDEX)].to_dict()\n",
    "title = _safe(row.get(\"Track Title\", row.get(\"title\", \"Unknown\")), \"Unknown\")\n",
    "artist = _safe(row.get(\"Artist Name\", row.get(\"artist_name\", \"Unknown\")), \"Unknown\")\n",
    "album = _safe(row.get(\"Album Title\", row.get(\"album_title\", \"\")), \"\")\n",
    "year = _safe(row.get(\"Release Year\", row.get(\"year\", \"\")), \"\")\n",
    "tags = _tags_text(row)\n",
    "rank = _safe(row.get(\"Deezer Rank\", row.get(\"deezer_rank\", \"\")), \"\")\n",
    "\n",
    "metadata_block = (\n",
    "    f\"Title: {title}\\n\"\n",
    "    f\"Artist: {artist}\\n\"\n",
    "    f\"Album: {album}\\n\"\n",
    "    f\"Release Year: {year}\\n\"\n",
    "    f\"Tags: {tags}\\n\"\n",
    "    f\"Popularity Rank: {rank}\\n\"\n",
    ")\n",
    "\n",
    "user_prompt = (\n",
    "    \"Return strict JSON only with keys: description, instrumental, confidence. \"\n",
    "    \"description must be 30-55 words, rich in sonic/mood/style cues, and must NOT mention title, artist, album, year, rank, or use quotes. \"\n",
    "    \"Use only provided metadata.\\n\\n\"\n",
    "    + metadata_block\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You generate factual music metadata JSON for recommendation systems.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"HF token detected: {bool(hf_token)}\")\n",
    "\n",
    "tokenizer_kwargs = {\"token\": hf_token} if hf_token else {}\n",
    "model_kwargs = {\"dtype\": dtype, \"device_map\": \"auto\"}\n",
    "if hf_token:\n",
    "    model_kwargs[\"token\"] = hf_token\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, **tokenizer_kwargs)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **model_kwargs)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "chat_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "inputs = tokenizer(chat_text, return_tensors=\"pt\")\n",
    "target_device = next(model.parameters()).device\n",
    "inputs = {k: v.to(target_device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=int(MAX_NEW_TOKENS),\n",
    "        do_sample=True,\n",
    "        temperature=float(TEMPERATURE),\n",
    "        top_p=float(TOP_P),\n",
    "        repetition_penalty=1.08,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "new_tokens = output[0, inputs[\"input_ids\"].shape[1]:]\n",
    "raw_output = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "parsed = _extract_first_json(raw_output) or {}\n",
    "description_preview = _strip_entities(parsed.get(\"description\", \"\"), [title, artist, album])\n",
    "if year:\n",
    "    description_preview = re.sub(rf\"\\b{re.escape(str(year))}\\b\", \"\", description_preview)\n",
    "    description_preview = re.sub(r\"\\s+\", \" \", description_preview).strip()\n",
    "\n",
    "print(\"\\n=== Preview Input ===\")\n",
    "print(metadata_block)\n",
    "print(\"=== Raw Model Output ===\")\n",
    "print(raw_output)\n",
    "print(\"=== Cleaned Description Preview ===\")\n",
    "print(description_preview)\n",
    "print(\"=== Instrumental Preview ===\")\n",
    "print({\n",
    "    \"instrumental\": parsed.get(\"instrumental\", None),\n",
    "    \"confidence\": parsed.get(\"confidence\", parsed.get(\"instrumental_confidence\", None)),\n",
    "})\n",
    "\n",
    "# Release preview model memory before enrichment cell.\n",
    "del model\n",
    "del tokenizer\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enrich CSV (Description + Instrumental)\n",
    "Pure Qwen mode: all description and instrumental fields are generated by Qwen2.5-3B (no local heuristic enrichment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "import hashlib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from src.core.media_metadata import as_optional_bool, clean_description\n",
    "\n",
    "RICH_CSV_PATH = str(RICH_CSV_PATH_DEFAULT)\n",
    "OVERWRITE_SOURCE_CSV = False\n",
    "\n",
    "# Pure Qwen enrichment with fast single-GPU execution options.\n",
    "ENABLE_LLM_ENRICHMENT = True\n",
    "LLM_MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "LLM_BACKEND = os.getenv(\"ERAEX_LLM_BACKEND\", \"auto\").strip().lower()  # auto | vllm | transformers\n",
    "LLM_MAX_ROWS = int(os.getenv(\"ERAEX_LLM_MAX_ROWS\", \"0\"))               # 0 = all rows in shard\n",
    "LLM_MAX_NEW_TOKENS = int(os.getenv(\"ERAEX_LLM_MAX_NEW_TOKENS\", \"120\"))\n",
    "LLM_TEMPERATURE = float(os.getenv(\"ERAEX_LLM_TEMPERATURE\", \"0.30\"))\n",
    "LLM_TOP_P = float(os.getenv(\"ERAEX_LLM_TOP_P\", \"0.90\"))\n",
    "LLM_REPETITION_PENALTY = float(os.getenv(\"ERAEX_LLM_REPETITION_PENALTY\", \"1.08\"))\n",
    "\n",
    "NUM_SHARDS = max(1, int(os.getenv(\"ERAEX_NUM_SHARDS\", \"1\")))\n",
    "SHARD_INDEX = int(os.getenv(\"ERAEX_SHARD_INDEX\", \"0\"))\n",
    "if SHARD_INDEX < 0 or SHARD_INDEX >= NUM_SHARDS:\n",
    "    raise ValueError(f\"Invalid shard settings: SHARD_INDEX={SHARD_INDEX}, NUM_SHARDS={NUM_SHARDS}\")\n",
    "\n",
    "\n",
    "def _run_nvidia_smi(query):\n",
    "    cmd = [\"nvidia-smi\", f\"--query-gpu={query}\", \"--format=csv,noheader,nounits\"]\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, text=True).strip().splitlines()\n",
    "        return out[0].strip() if out else \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _gpu_vram_gb():\n",
    "    total_mb = _run_nvidia_smi(\"memory.total\")\n",
    "    try:\n",
    "        return float(total_mb) / 1024.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def _gpu_name():\n",
    "    return _run_nvidia_smi(\"name\") or \"unknown-gpu\"\n",
    "\n",
    "\n",
    "GPU_VRAM_GB = _gpu_vram_gb()\n",
    "DEFAULT_TF_BATCH_SIZE = (\n",
    "    24 if GPU_VRAM_GB >= 90 else\n",
    "    16 if GPU_VRAM_GB >= 60 else\n",
    "    12 if GPU_VRAM_GB >= 40 else\n",
    "    8 if GPU_VRAM_GB >= 24 else\n",
    "    4 if GPU_VRAM_GB >= 16 else 2\n",
    ")\n",
    "DEFAULT_VLLM_MAX_NUM_SEQS = (\n",
    "    256 if GPU_VRAM_GB >= 90 else\n",
    "    192 if GPU_VRAM_GB >= 60 else\n",
    "    128 if GPU_VRAM_GB >= 40 else\n",
    "    96 if GPU_VRAM_GB >= 24 else 64\n",
    ")\n",
    "DEFAULT_VLLM_PROMPT_BATCH_SIZE = (\n",
    "    1024 if GPU_VRAM_GB >= 90 else\n",
    "    768 if GPU_VRAM_GB >= 60 else\n",
    "    512 if GPU_VRAM_GB >= 40 else 256\n",
    ")\n",
    "\n",
    "TRANSFORMERS_BATCH_SIZE = int(os.getenv(\"ERAEX_TF_BATCH_SIZE\", str(DEFAULT_TF_BATCH_SIZE)))\n",
    "VLLM_MAX_NUM_SEQS = int(os.getenv(\"ERAEX_VLLM_MAX_NUM_SEQS\", str(DEFAULT_VLLM_MAX_NUM_SEQS)))\n",
    "VLLM_PROMPT_BATCH_SIZE = int(os.getenv(\"ERAEX_VLLM_PROMPT_BATCH_SIZE\", str(DEFAULT_VLLM_PROMPT_BATCH_SIZE)))\n",
    "VLLM_GPU_MEMORY_UTIL = float(os.getenv(\"ERAEX_VLLM_GPU_MEM_UTIL\", \"0.92\"))\n",
    "VLLM_MAX_MODEL_LEN = int(os.getenv(\"ERAEX_VLLM_MAX_MODEL_LEN\", \"8192\"))\n",
    "\n",
    "CACHE_PATH = Path(INDEX_DIR) / \"qwen_cache.jsonl\"\n",
    "CACHE_FLUSH_EVERY = int(os.getenv(\"ERAEX_CACHE_FLUSH_EVERY\", \"500\"))\n",
    "\n",
    "\n",
    "def _get_hf_token():\n",
    "    return (\n",
    "        os.getenv(\"HF_TOKEN\")\n",
    "        or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "        or None\n",
    "    )\n",
    "\n",
    "\n",
    "def _release_gpu_memory():\n",
    "    # Free any previously loaded transformer objects from earlier notebook cells.\n",
    "    for name in [\"model\", \"tokenizer\", \"_tf_model\", \"_tf_tokenizer\", \"_llm_model\", \"_llm_tokenizer\"]:\n",
    "        if name in globals():\n",
    "            try:\n",
    "                del globals()[name]\n",
    "            except Exception:\n",
    "                pass\n",
    "    gc.collect()\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def _resolve_backend(preferred):\n",
    "    if preferred not in {\"auto\", \"vllm\", \"transformers\"}:\n",
    "        raise ValueError(f\"Unsupported backend: {preferred}\")\n",
    "    if preferred in {\"auto\", \"vllm\"}:\n",
    "        try:\n",
    "            import vllm  # noqa: F401\n",
    "            return \"vllm\"\n",
    "        except Exception as exc:\n",
    "            if preferred == \"vllm\":\n",
    "                raise RuntimeError(\"vLLM requested but unavailable. Install with `%pip install vllm`.\") from exc\n",
    "            print(f\"[LLM] vLLM unavailable ({exc}); falling back to transformers\")\n",
    "    return \"transformers\"\n",
    "\n",
    "\n",
    "def _pick(row, keys, default=\"\"):\n",
    "    for key in keys:\n",
    "        value = row.get(key, \"\")\n",
    "        if value is None:\n",
    "            continue\n",
    "        if isinstance(value, float) and pd.isna(value):\n",
    "            continue\n",
    "        text = str(value).strip()\n",
    "        if text and text.lower() != \"nan\":\n",
    "            return value\n",
    "    return default\n",
    "\n",
    "\n",
    "def _parse_tags(raw_tags):\n",
    "    if isinstance(raw_tags, list):\n",
    "        return [str(tag) for tag in raw_tags if str(tag).strip()]\n",
    "    if not isinstance(raw_tags, str):\n",
    "        return []\n",
    "    text = raw_tags.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        parsed = json.loads(text.replace(\"'\", '\"'))\n",
    "        if isinstance(parsed, list):\n",
    "            return [str(tag) for tag in parsed if str(tag).strip()]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "\n",
    "def _extract_first_json(text):\n",
    "    text = str(text or \"\").strip()\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "    match = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(match.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _strip_entities(text, entities):\n",
    "    cleaned = str(text or \"\")\n",
    "    for ent in entities:\n",
    "        ent = str(ent or \"\").strip()\n",
    "        if len(ent) < 4:\n",
    "            continue\n",
    "        cleaned = re.sub(re.escape(ent), \"\", cleaned, flags=re.IGNORECASE)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", cleaned)\n",
    "    cleaned = re.sub(r\"^[,;:.\\- ]+\", \"\", cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "\n",
    "def _clean_generated_description(raw_desc, row_dict):\n",
    "    title = str(_pick(row_dict, [\"Track Title\", \"title\"], \"\")).strip()\n",
    "    artist = str(_pick(row_dict, [\"Artist Name\", \"artist_name\"], \"\")).strip()\n",
    "    album = str(_pick(row_dict, [\"Album Title\", \"album_title\"], \"\")).strip()\n",
    "    year = str(_pick(row_dict, [\"Release Year\", \"year\"], \"\")).strip()\n",
    "\n",
    "    desc = str(raw_desc or \"\").strip().strip('\"').strip(\"'\")\n",
    "    desc = _strip_entities(desc, [title, artist, album])\n",
    "    if year:\n",
    "        desc = re.sub(rf\"\\b{re.escape(year)}\\b\", \"\", desc)\n",
    "    desc = re.sub(r\"\\s+\", \" \", desc).strip()\n",
    "    desc = clean_description(desc, max_chars=280)\n",
    "    return desc\n",
    "\n",
    "\n",
    "def _prompt_fields(row_dict):\n",
    "    tags = [str(t).strip().lower() for t in _parse_tags(_pick(row_dict, [\"Deezer Tags\", \"deezer_tags\"], \"[]\"))]\n",
    "    tags = [t for t in tags if t][:16]\n",
    "    return {\n",
    "        \"title\": str(_pick(row_dict, [\"Track Title\", \"title\"], \"Unknown\")).strip().lower(),\n",
    "        \"artist\": str(_pick(row_dict, [\"Artist Name\", \"artist_name\"], \"Unknown\")).strip().lower(),\n",
    "        \"album\": str(_pick(row_dict, [\"Album Title\", \"album_title\"], \"\")).strip().lower(),\n",
    "        \"year\": str(_pick(row_dict, [\"Release Year\", \"year\"], \"\")).strip(),\n",
    "        \"rank\": str(_pick(row_dict, [\"Deezer Rank\", \"deezer_rank\", \"rank\"], \"\")).strip(),\n",
    "        \"tags\": tags,\n",
    "    }\n",
    "\n",
    "\n",
    "def _prompt_key(row_dict):\n",
    "    payload = _prompt_fields(row_dict)\n",
    "    raw = json.dumps(payload, ensure_ascii=False, sort_keys=True, separators=(\",\", \":\"))\n",
    "    return hashlib.sha1(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def _build_llm_messages(row_dict):\n",
    "    title = str(_pick(row_dict, [\"Track Title\", \"title\"], \"Unknown\")).strip() or \"Unknown\"\n",
    "    artist = str(_pick(row_dict, [\"Artist Name\", \"artist_name\"], \"Unknown\")).strip() or \"Unknown\"\n",
    "    album = str(_pick(row_dict, [\"Album Title\", \"album_title\"], \"\")).strip()\n",
    "    year = str(_pick(row_dict, [\"Release Year\", \"year\"], \"\")).strip()\n",
    "    rank = str(_pick(row_dict, [\"Deezer Rank\", \"deezer_rank\", \"rank\"], \"\")).strip()\n",
    "    tags = _parse_tags(_pick(row_dict, [\"Deezer Tags\", \"deezer_tags\"], \"[]\"))\n",
    "    tags_text = \", \".join(tags[:12])\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Return strict JSON only with keys: description, instrumental, confidence. \"\n",
    "        \"description must be 30-55 words, rich in sonic/mood/style cues, and must NOT mention title, artist, album, year, rank, or use quotes. \"\n",
    "        \"Use only provided metadata and avoid hallucinations.\\n\\n\"\n",
    "        f\"Title: {title}\\n\"\n",
    "        f\"Artist: {artist}\\n\"\n",
    "        f\"Album: {album}\\n\"\n",
    "        f\"Release Year: {year}\\n\"\n",
    "        f\"Tags: {tags_text}\\n\"\n",
    "        f\"Popularity Rank: {rank}\\n\"\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You generate factual music metadata JSON for recommendation systems.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "def _parse_payload_from_raw(raw_output, row_dict):\n",
    "    parsed = _extract_first_json(raw_output)\n",
    "    if not isinstance(parsed, dict):\n",
    "        return None\n",
    "\n",
    "    desc_new = _clean_generated_description(parsed.get(\"description\", \"\"), row_dict)\n",
    "    inst_new = as_optional_bool(parsed.get(\"instrumental\"))\n",
    "\n",
    "    conf_val = parsed.get(\"instrumental_confidence\", parsed.get(\"confidence\", None))\n",
    "    try:\n",
    "        conf_new = float(conf_val) if conf_val is not None else None\n",
    "    except Exception:\n",
    "        conf_new = None\n",
    "    if conf_new is not None:\n",
    "        if conf_new > 1.0:\n",
    "            conf_new = conf_new / 100.0\n",
    "        conf_new = min(1.0, max(0.0, conf_new))\n",
    "\n",
    "    return {\n",
    "        \"description\": desc_new,\n",
    "        \"instrumental\": inst_new,\n",
    "        \"instrumental_confidence\": conf_new,\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_cache(path):\n",
    "    cache = {}\n",
    "    if not path.exists():\n",
    "        return cache\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            key = str(obj.get(\"key\", \"\")).strip()\n",
    "            if not key:\n",
    "                continue\n",
    "            cache[key] = {\n",
    "                \"description\": str(obj.get(\"description\", \"\") or \"\"),\n",
    "                \"instrumental\": obj.get(\"instrumental\", None),\n",
    "                \"instrumental_confidence\": obj.get(\"instrumental_confidence\", None),\n",
    "            }\n",
    "    return cache\n",
    "\n",
    "\n",
    "def _append_cache_entries(path, entries):\n",
    "    if not entries:\n",
    "        return\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for key, payload in entries:\n",
    "            record = {\n",
    "                \"key\": key,\n",
    "                \"description\": payload.get(\"description\", \"\"),\n",
    "                \"instrumental\": payload.get(\"instrumental\", None),\n",
    "                \"instrumental_confidence\": payload.get(\"instrumental_confidence\", None),\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def _apply_payload_to_row(idx, payload, source):\n",
    "    changed = False\n",
    "\n",
    "    desc_new = clean_description(str(payload.get(\"description\", \"\") or \"\"), max_chars=280)\n",
    "    if desc_new:\n",
    "        df_rich.at[idx, \"Description\"] = desc_new\n",
    "        changed = True\n",
    "\n",
    "    inst_new = as_optional_bool(payload.get(\"instrumental\"))\n",
    "    if inst_new is not None:\n",
    "        df_rich.at[idx, \"Instrumental\"] = bool(inst_new)\n",
    "        changed = True\n",
    "\n",
    "    conf_val = payload.get(\"instrumental_confidence\", None)\n",
    "    try:\n",
    "        conf_new = float(conf_val) if conf_val is not None else None\n",
    "    except Exception:\n",
    "        conf_new = None\n",
    "    if conf_new is not None:\n",
    "        if conf_new > 1.0:\n",
    "            conf_new = conf_new / 100.0\n",
    "        conf_new = min(1.0, max(0.0, conf_new))\n",
    "        df_rich.at[idx, \"Instrumental Confidence\"] = float(conf_new)\n",
    "        changed = True\n",
    "\n",
    "    df_rich.at[idx, \"Description Source\"] = source if changed else \"qwen-empty\"\n",
    "    return changed\n",
    "\n",
    "\n",
    "def _load_transformers_model_once():\n",
    "    global _tf_tokenizer, _tf_model\n",
    "    if \"_tf_tokenizer\" in globals() and \"_tf_model\" in globals():\n",
    "        return _tf_tokenizer, _tf_model\n",
    "\n",
    "    hf_token = _get_hf_token()\n",
    "    dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    tokenizer_kwargs = {\"token\": hf_token} if hf_token else {}\n",
    "    model_kwargs = {\"dtype\": dtype, \"device_map\": \"auto\"}\n",
    "    if hf_token:\n",
    "        model_kwargs[\"token\"] = hf_token\n",
    "\n",
    "    _tf_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID, **tokenizer_kwargs)\n",
    "    _tf_model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_ID, **model_kwargs)\n",
    "    if _tf_tokenizer.pad_token is None:\n",
    "        _tf_tokenizer.pad_token = _tf_tokenizer.eos_token\n",
    "\n",
    "    return _tf_tokenizer, _tf_model\n",
    "\n",
    "\n",
    "# Initialize output dataframe for pure Qwen population.\n",
    "df_rich = df.copy()\n",
    "for legacy_col in [\"description\", \"Track Description\", \"YouTube Description\"]:\n",
    "    if legacy_col in df_rich.columns:\n",
    "        df_rich = df_rich.drop(columns=[legacy_col])\n",
    "\n",
    "df_rich[\"Track ID\"] = df_rich.apply(\n",
    "    lambda r: str(_pick(r.to_dict(), [\"Track ID\", \"deezer_id\", \"video_id\"], \"\")).strip(),\n",
    "    axis=1,\n",
    ")\n",
    "df_rich[\"Description\"] = \"\"\n",
    "df_rich[\"Instrumental\"] = pd.NA\n",
    "df_rich[\"Instrumental Confidence\"] = 0.0\n",
    "df_rich[\"Description Source\"] = \"qwen-pending\"\n",
    "\n",
    "\n",
    "rows_updated = 0\n",
    "rows_failed = 0\n",
    "rows_cache_hits = 0\n",
    "unique_generated = 0\n",
    "\n",
    "if ENABLE_LLM_ENRICHMENT:\n",
    "    backend = _resolve_backend(LLM_BACKEND)\n",
    "    hf_token = _get_hf_token()\n",
    "\n",
    "    print(f\"LLM model={LLM_MODEL_ID}\")\n",
    "    print(f\"backend={backend} | GPU_VRAM_GB={GPU_VRAM_GB:.1f}\")\n",
    "    print(f\"CUDA device: {_gpu_name()}\")\n",
    "    print(f\"HF token detected: {bool(hf_token)}\")\n",
    "    print(f\"shard={SHARD_INDEX + 1}/{NUM_SHARDS}\")\n",
    "\n",
    "    all_indices = list(df_rich.index)\n",
    "    shard_indices = [idx for pos, idx in enumerate(all_indices) if pos % NUM_SHARDS == SHARD_INDEX]\n",
    "    if LLM_MAX_ROWS > 0:\n",
    "        shard_indices = shard_indices[:LLM_MAX_ROWS]\n",
    "\n",
    "    key_to_indices = {}\n",
    "    key_to_row = {}\n",
    "    for idx in tqdm(shard_indices, desc=\"Preparing prompt keys\"):\n",
    "        row_dict = df_rich.loc[idx].to_dict()\n",
    "        key = _prompt_key(row_dict)\n",
    "        key_to_indices.setdefault(key, []).append(idx)\n",
    "        if key not in key_to_row:\n",
    "            key_to_row[key] = row_dict\n",
    "\n",
    "    unique_keys = list(key_to_indices.keys())\n",
    "    cache = _load_cache(CACHE_PATH)\n",
    "    pending_keys = []\n",
    "\n",
    "    for key in unique_keys:\n",
    "        cached_payload = cache.get(key)\n",
    "        if cached_payload is None:\n",
    "            pending_keys.append(key)\n",
    "            continue\n",
    "        for idx in key_to_indices[key]:\n",
    "            ok = _apply_payload_to_row(idx, cached_payload, \"qwen-cache\")\n",
    "            rows_updated += int(ok)\n",
    "            rows_failed += int(not ok)\n",
    "            rows_cache_hits += 1\n",
    "\n",
    "    print(\n",
    "        f\"target_rows={len(shard_indices)} | unique_prompts={len(unique_keys)} | \"\n",
    "        f\"cache_hits_rows={rows_cache_hits} | pending_unique_prompts={len(pending_keys)}\"\n",
    "    )\n",
    "\n",
    "    cache_buffer = []\n",
    "    run_start = time.time()\n",
    "\n",
    "    if backend == \"vllm\":\n",
    "        from vllm import LLM, SamplingParams\n",
    "\n",
    "        os.environ.setdefault(\"HF_TOKEN\", hf_token)\n",
    "        os.environ.setdefault(\"HUGGINGFACE_HUB_TOKEN\", hf_token)\n",
    "        os.environ.setdefault(\"VLLM_WORKER_MULTIPROC_METHOD\", \"spawn\")\n",
    "\n",
    "        chat_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID, token=hf_token)\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=float(LLM_TEMPERATURE),\n",
    "            top_p=float(LLM_TOP_P),\n",
    "            max_tokens=int(LLM_MAX_NEW_TOKENS),\n",
    "            repetition_penalty=float(LLM_REPETITION_PENALTY),\n",
    "        )\n",
    "\n",
    "        _release_gpu_memory()\n",
    "\n",
    "        init_candidates = [\n",
    "            (int(VLLM_MAX_NUM_SEQS), float(VLLM_GPU_MEMORY_UTIL), int(VLLM_MAX_MODEL_LEN)),\n",
    "            (max(64, int(VLLM_MAX_NUM_SEQS * 0.75)), min(float(VLLM_GPU_MEMORY_UTIL), 0.88), int(VLLM_MAX_MODEL_LEN)),\n",
    "            (max(64, int(VLLM_MAX_NUM_SEQS * 0.50)), min(float(VLLM_GPU_MEMORY_UTIL), 0.84), min(int(VLLM_MAX_MODEL_LEN), 6144)),\n",
    "            (64, min(float(VLLM_GPU_MEMORY_UTIL), 0.80), min(int(VLLM_MAX_MODEL_LEN), 4096)),\n",
    "        ]\n",
    "\n",
    "        dedup_init_candidates = []\n",
    "        seen = set()\n",
    "        for cand in init_candidates:\n",
    "            key = (int(cand[0]), round(float(cand[1]), 3), int(cand[2]))\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            dedup_init_candidates.append(cand)\n",
    "\n",
    "        vllm_engine = None\n",
    "        active_max_num_seqs = None\n",
    "        active_gpu_mem_util = None\n",
    "        active_max_model_len = None\n",
    "        vllm_init_error = None\n",
    "\n",
    "        for cand_max_num_seqs, cand_gpu_mem, cand_max_model_len in dedup_init_candidates:\n",
    "            try:\n",
    "                print(\n",
    "                    f\"[LLM] vLLM init attempt: max_num_seqs={cand_max_num_seqs}, \"\n",
    "                    f\"gpu_mem_util={cand_gpu_mem:.2f}, max_model_len={cand_max_model_len}\"\n",
    "                )\n",
    "                vllm_engine = LLM(\n",
    "                    model=LLM_MODEL_ID,\n",
    "                    tensor_parallel_size=1,\n",
    "                    dtype=\"float16\",\n",
    "                    gpu_memory_utilization=float(cand_gpu_mem),\n",
    "                    max_num_seqs=int(cand_max_num_seqs),\n",
    "                    max_model_len=int(cand_max_model_len),\n",
    "                )\n",
    "                active_max_num_seqs = int(cand_max_num_seqs)\n",
    "                active_gpu_mem_util = float(cand_gpu_mem)\n",
    "                active_max_model_len = int(cand_max_model_len)\n",
    "                break\n",
    "            except Exception as exc:\n",
    "                vllm_init_error = exc\n",
    "                print(f\"[LLM] vLLM init failed for this config: {exc}\")\n",
    "                time.sleep(1)\n",
    "\n",
    "        if vllm_engine is not None:\n",
    "            effective_prompt_batch = max(64, min(int(VLLM_PROMPT_BATCH_SIZE), int(active_max_num_seqs) * 4))\n",
    "            print(\n",
    "                f\"vLLM config: max_num_seqs={active_max_num_seqs}, \"\n",
    "                f\"prompt_batch_size={effective_prompt_batch}, gpu_mem_util={active_gpu_mem_util}, \"\n",
    "                f\"max_model_len={active_max_model_len}\"\n",
    "            )\n",
    "\n",
    "            for start in tqdm(range(0, len(pending_keys), effective_prompt_batch), desc=\"qwen enrichment (vllm)\"):\n",
    "                batch_keys = pending_keys[start : start + effective_prompt_batch]\n",
    "                prompts = [\n",
    "                    chat_tokenizer.apply_chat_template(\n",
    "                        _build_llm_messages(key_to_row[key]),\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True,\n",
    "                    )\n",
    "                    for key in batch_keys\n",
    "                ]\n",
    "                outputs = vllm_engine.generate(prompts, sampling_params=sampling_params, use_tqdm=False)\n",
    "\n",
    "                for key, out in zip(batch_keys, outputs):\n",
    "                    raw = \"\"\n",
    "                    if out.outputs:\n",
    "                        raw = str(out.outputs[0].text or \"\").strip()\n",
    "\n",
    "                    payload = _parse_payload_from_raw(raw, key_to_row[key])\n",
    "                    if payload is None:\n",
    "                        for idx in key_to_indices[key]:\n",
    "                            df_rich.at[idx, \"Description Source\"] = \"qwen-parse-failed\"\n",
    "                            rows_failed += 1\n",
    "                        continue\n",
    "\n",
    "                    unique_generated += 1\n",
    "                    cache_buffer.append((key, payload))\n",
    "\n",
    "                    for idx in key_to_indices[key]:\n",
    "                        ok = _apply_payload_to_row(idx, payload, \"qwen2.5-3b\")\n",
    "                        rows_updated += int(ok)\n",
    "                        rows_failed += int(not ok)\n",
    "\n",
    "                if len(cache_buffer) >= CACHE_FLUSH_EVERY:\n",
    "                    _append_cache_entries(CACHE_PATH, cache_buffer)\n",
    "                    cache_buffer = []\n",
    "        else:\n",
    "            print(f\"[LLM] vLLM init failed after retries ({vllm_init_error}); switching to transformers backend\")\n",
    "            backend = \"transformers\"\n",
    "\n",
    "    if backend != \"vllm\":\n",
    "        tf_tokenizer, tf_model = _load_transformers_model_once()\n",
    "        batch_size = max(1, int(TRANSFORMERS_BATCH_SIZE))\n",
    "        print(f\"transformers batch_size={batch_size}\")\n",
    "\n",
    "        for start in tqdm(range(0, len(pending_keys), batch_size), desc=\"qwen enrichment (transformers)\"):\n",
    "            batch_keys = pending_keys[start : start + batch_size]\n",
    "            row_dicts = [key_to_row[key] for key in batch_keys]\n",
    "            messages_batch = [_build_llm_messages(row_dict) for row_dict in row_dicts]\n",
    "            chat_texts = [\n",
    "                tf_tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "                for msgs in messages_batch\n",
    "            ]\n",
    "\n",
    "            encoded = tf_tokenizer(\n",
    "                chat_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=768,\n",
    "            )\n",
    "            input_lens = encoded[\"attention_mask\"].sum(dim=1)\n",
    "            target_device = next(tf_model.parameters()).device\n",
    "            encoded = {k: v.to(target_device) for k, v in encoded.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = tf_model.generate(\n",
    "                    **encoded,\n",
    "                    max_new_tokens=int(LLM_MAX_NEW_TOKENS),\n",
    "                    do_sample=True,\n",
    "                    temperature=float(LLM_TEMPERATURE),\n",
    "                    top_p=float(LLM_TOP_P),\n",
    "                    repetition_penalty=float(LLM_REPETITION_PENALTY),\n",
    "                    pad_token_id=tf_tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            for pos, key in enumerate(batch_keys):\n",
    "                new_tokens = outputs[pos, int(input_lens[pos]) :]\n",
    "                raw = tf_tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "                payload = _parse_payload_from_raw(raw, key_to_row[key])\n",
    "                if payload is None:\n",
    "                    for idx in key_to_indices[key]:\n",
    "                        df_rich.at[idx, \"Description Source\"] = \"qwen-parse-failed\"\n",
    "                        rows_failed += 1\n",
    "                    continue\n",
    "\n",
    "                unique_generated += 1\n",
    "                cache_buffer.append((key, payload))\n",
    "\n",
    "                for idx in key_to_indices[key]:\n",
    "                    ok = _apply_payload_to_row(idx, payload, \"qwen2.5-3b\")\n",
    "                    rows_updated += int(ok)\n",
    "                    rows_failed += int(not ok)\n",
    "\n",
    "            if len(cache_buffer) >= CACHE_FLUSH_EVERY:\n",
    "                _append_cache_entries(CACHE_PATH, cache_buffer)\n",
    "                cache_buffer = []\n",
    "\n",
    "    _append_cache_entries(CACHE_PATH, cache_buffer)\n",
    "\n",
    "    elapsed_h = (time.time() - run_start) / 3600.0\n",
    "    print(\n",
    "        f\"rows_updated={rows_updated}, rows_failed={rows_failed}, rows_cache_hits={rows_cache_hits}, \"\n",
    "        f\"unique_generated={unique_generated}, elapsed={elapsed_h:.2f}h\"\n",
    "    )\n",
    "else:\n",
    "    print(\"LLM enrichment disabled\")\n",
    "\n",
    "\n",
    "df_rich.to_csv(RICH_CSV_PATH, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved rich CSV: {RICH_CSV_PATH} rows={len(df_rich)}\")\n",
    "if OVERWRITE_SOURCE_CSV:\n",
    "    df_rich.to_csv(DATA_PATH, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Overwrote source CSV: {DATA_PATH}\")\n",
    "\n",
    "sync_path(Path(RICH_CSV_PATH), \"EraEx_Dataset_Rich.csv\")\n",
    "print(f\"rows_updated={rows_updated}, rows_failed={rows_failed}, rows_cache_hits={rows_cache_hits}\")\n",
    "df_rich[[\"Track ID\", \"Track Title\", \"Artist Name\", \"Description\", \"Instrumental\", \"Instrumental Confidence\", \"Description Source\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build `id_map.json` + `metadata.json` from Rich CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"df_rich\" not in globals():\n",
    "    df_rich = pd.read_csv(RICH_CSV_PATH, low_memory=False)\n",
    "\n",
    "\n",
    "def _parse_tags_from_row(row):\n",
    "    raw = row.get(\"Deezer Tags\", row.get(\"deezer_tags\", \"[]\"))\n",
    "    if isinstance(raw, list):\n",
    "        return [str(tag).strip() for tag in raw if str(tag).strip()]\n",
    "    if not isinstance(raw, str):\n",
    "        return []\n",
    "    text = raw.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    try:\n",
    "        parsed = json.loads(text.replace(\"'\", '\"'))\n",
    "        if isinstance(parsed, list):\n",
    "            return [str(tag).strip() for tag in parsed if str(tag).strip()]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "\n",
    "def _safe_float(value, default=0.0):\n",
    "    try:\n",
    "        num = float(value)\n",
    "        if pd.isna(num):\n",
    "            return float(default)\n",
    "        return float(num)\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "\n",
    "def _safe_unit_float(value, default=0.0):\n",
    "    num = _safe_float(value, default=default)\n",
    "    if num > 1.0:\n",
    "        num = num / 100.0\n",
    "    return float(min(1.0, max(0.0, num)))\n",
    "\n",
    "\n",
    "def _derive_vibe_tags(tags, tempo, energy, brightness, mood, valence):\n",
    "    vibes = set()\n",
    "    for tag in tags:\n",
    "        for tok in re.findall(r\"[a-z0-9]+\", str(tag).lower()):\n",
    "            if tok:\n",
    "                vibes.add(tok)\n",
    "\n",
    "    if mood >= 0.68:\n",
    "        vibes.add(\"moody\")\n",
    "    if valence >= 0.66:\n",
    "        vibes.add(\"uplifting\")\n",
    "    elif valence <= 0.36:\n",
    "        vibes.add(\"melancholic\")\n",
    "    if energy >= 0.66:\n",
    "        vibes.add(\"energetic\")\n",
    "    elif energy <= 0.38:\n",
    "        vibes.add(\"chill\")\n",
    "    if tempo >= 0.70:\n",
    "        vibes.add(\"fast\")\n",
    "    elif tempo <= 0.38:\n",
    "        vibes.add(\"slow\")\n",
    "    if brightness <= 0.35:\n",
    "        vibes.add(\"dark\")\n",
    "    elif brightness >= 0.68:\n",
    "        vibes.add(\"bright\")\n",
    "\n",
    "    ordered = sorted(vibes)\n",
    "    return ordered[:24]\n",
    "\n",
    "\n",
    "ids = []\n",
    "metadata = {}\n",
    "\n",
    "for _, row in tqdm(df_rich.iterrows(), total=len(df_rich), desc=\"Building metadata\"):\n",
    "    row = row.to_dict()\n",
    "    track_id = str(row.get(\"Track ID\", \"\")).strip()\n",
    "    if not track_id:\n",
    "        continue\n",
    "\n",
    "    title = str(row.get(\"Track Title\", row.get(\"title\", \"Unknown\"))).strip() or \"Unknown\"\n",
    "    artist = str(row.get(\"Artist Name\", row.get(\"artist_name\", \"Unknown\"))).strip() or \"Unknown\"\n",
    "    album = str(row.get(\"Album Title\", row.get(\"album_title\", \"Unknown\"))).strip() or \"Unknown\"\n",
    "    tags = _parse_tags_from_row(row)\n",
    "\n",
    "    try:\n",
    "        year = int(float(row.get(\"Release Year\", row.get(\"year\", 0)) or 0))\n",
    "    except Exception:\n",
    "        year = 0\n",
    "    try:\n",
    "        playcount = int(float(row.get(\"Play Count\", row.get(\"deezer_playcount\", 0)) or 0))\n",
    "    except Exception:\n",
    "        playcount = 0\n",
    "    try:\n",
    "        deezer_rank = int(float(row.get(\"Deezer Rank\", row.get(\"deezer_rank\", row.get(\"rank\", 0))) or 0))\n",
    "    except Exception:\n",
    "        deezer_rank = 0\n",
    "    try:\n",
    "        views = int(float(row.get(\"Views\", row.get(\"views\", 0)) or 0))\n",
    "    except Exception:\n",
    "        views = 0\n",
    "\n",
    "    tempo = _safe_unit_float(row.get(\"tempo\", row.get(\"Tempo\", 0.0)), default=0.0)\n",
    "    energy = _safe_unit_float(row.get(\"energy\", row.get(\"Energy\", 0.0)), default=0.0)\n",
    "    brightness = _safe_unit_float(row.get(\"brightness\", row.get(\"Brightness\", 0.0)), default=0.0)\n",
    "    mood = _safe_unit_float(row.get(\"mood\", row.get(\"Mood\", 0.0)), default=0.0)\n",
    "    valence = _safe_unit_float(row.get(\"valence\", row.get(\"Valence\", 0.0)), default=0.0)\n",
    "\n",
    "    description = clean_description(str(row.get(\"Description\", \"\") or \"\"), max_chars=280)\n",
    "    instrumental = as_optional_bool(row.get(\"Instrumental\"))\n",
    "    confidence = _safe_unit_float(row.get(\"Instrumental Confidence\", 0.0), default=0.0)\n",
    "\n",
    "    vibe_tags = _derive_vibe_tags(tags, tempo, energy, brightness, mood, valence)\n",
    "\n",
    "    ids.append(track_id)\n",
    "    metadata[track_id] = {\n",
    "        \"id\": track_id,\n",
    "        \"video_id\": track_id,\n",
    "        \"title\": title,\n",
    "        \"name\": title,\n",
    "        \"artist_name\": artist,\n",
    "        \"album_title\": album,\n",
    "        \"description\": description,\n",
    "        \"description_source\": str(row.get(\"Description Source\", \"\") or \"\"),\n",
    "        \"instrumental\": instrumental,\n",
    "        \"instrumental_confidence\": float(confidence),\n",
    "        \"year\": year,\n",
    "        \"release_date\": f\"{year}-01-01\" if year else \"\",\n",
    "        \"deezer_tags\": tags,\n",
    "        \"genres\": tags,\n",
    "        \"vibe_tags\": vibe_tags,\n",
    "        \"tempo\": float(tempo),\n",
    "        \"energy\": float(energy),\n",
    "        \"brightness\": float(brightness),\n",
    "        \"mood\": float(mood),\n",
    "        \"valence\": float(valence),\n",
    "        \"deezer_playcount\": playcount,\n",
    "        \"deezer_rank\": deezer_rank,\n",
    "        \"rank\": deezer_rank,\n",
    "        \"views\": views,\n",
    "        \"cover_url\": str(row.get(\"Cover URL\", row.get(\"cover_url\", \"\")) or \"\").strip(),\n",
    "    }\n",
    "\n",
    "index_dir = Path(INDEX_DIR)\n",
    "index_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(index_dir / \"id_map.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ids, f)\n",
    "with open(index_dir / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False)\n",
    "\n",
    "sync_path(index_dir / \"id_map.json\", \"indexes/id_map.json\")\n",
    "sync_path(index_dir / \"metadata.json\", \"indexes/metadata.json\")\n",
    "print(f\"Saved id_map.json ({len(ids)}) and metadata.json ({len(metadata)})\")\n",
    "if ids:\n",
    "    sample_meta = metadata[str(ids[0])]\n",
    "    print(\"Sample metadata keys:\", sorted(sample_meta.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Context-First BGE-M3 Embeddings (Genre/Tags/Vibe/Description/Audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from src.core.media_metadata import as_optional_bool, clean_description\n",
    "\n",
    "# Suggest embedding batch size from available VRAM.\n",
    "def _suggest_embed_batch_size(vram_gb):\n",
    "    if vram_gb >= 40:\n",
    "        return 256\n",
    "    if vram_gb >= 24:\n",
    "        return 160\n",
    "    if vram_gb >= 16:\n",
    "        return 96\n",
    "    if vram_gb >= 10:\n",
    "        return 64\n",
    "    return 32\n",
    "\n",
    "\n",
    "def _as_list(value):\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        text = value.strip()\n",
    "        if not text:\n",
    "            return []\n",
    "        try:\n",
    "            parsed = json.loads(text.replace(\"'\", '\"'))\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "        except Exception:\n",
    "            pass\n",
    "    return []\n",
    "\n",
    "\n",
    "def _safe_unit_float(value, default=0.0):\n",
    "    try:\n",
    "        num = float(value)\n",
    "    except Exception:\n",
    "        num = float(default)\n",
    "    if num > 1.0:\n",
    "        num = num / 100.0\n",
    "    return float(min(1.0, max(0.0, num)))\n",
    "\n",
    "\n",
    "def _bucket(v, low=0.35, high=0.67):\n",
    "    if v < low:\n",
    "        return \"low\"\n",
    "    if v > high:\n",
    "        return \"high\"\n",
    "    return \"medium\"\n",
    "\n",
    "\n",
    "def _audio_vibe_tokens(tempo, energy, brightness, mood, valence):\n",
    "    vibes = []\n",
    "    if mood >= 0.68:\n",
    "        vibes.append(\"moody\")\n",
    "    if valence >= 0.66:\n",
    "        vibes.append(\"uplifting\")\n",
    "    elif valence <= 0.36:\n",
    "        vibes.append(\"melancholic\")\n",
    "    if energy >= 0.66:\n",
    "        vibes.append(\"energetic\")\n",
    "    elif energy <= 0.38:\n",
    "        vibes.append(\"chill\")\n",
    "    if tempo >= 0.70:\n",
    "        vibes.append(\"fast\")\n",
    "    elif tempo <= 0.38:\n",
    "        vibes.append(\"slow\")\n",
    "    if brightness <= 0.35:\n",
    "        vibes.append(\"dark\")\n",
    "    elif brightness >= 0.68:\n",
    "        vibes.append(\"bright\")\n",
    "    dedup = []\n",
    "    seen = set()\n",
    "    for v in vibes:\n",
    "        if v not in seen:\n",
    "            dedup.append(v)\n",
    "            seen.add(v)\n",
    "    return dedup\n",
    "\n",
    "\n",
    "# Build one context-first embedding text per track id from metadata.\n",
    "def _build_text_for_track(track_id):\n",
    "    meta = metadata.get(str(track_id), {})\n",
    "\n",
    "    title = str(meta.get(\"title\", meta.get(\"name\", \"Unknown\")) or \"Unknown\").strip() or \"Unknown\"\n",
    "    artist = str(meta.get(\"artist_name\", \"Unknown\") or \"Unknown\").strip() or \"Unknown\"\n",
    "\n",
    "    tags = [str(t).strip().lower() for t in _as_list(meta.get(\"deezer_tags\", [])) if str(t).strip()]\n",
    "    genres = [str(g).strip().lower() for g in _as_list(meta.get(\"genres\", [])) if str(g).strip()]\n",
    "    vibe_tags = [str(v).strip().lower() for v in _as_list(meta.get(\"vibe_tags\", [])) if str(v).strip()]\n",
    "\n",
    "    tempo = _safe_unit_float(meta.get(\"tempo\", 0.0), default=0.0)\n",
    "    energy = _safe_unit_float(meta.get(\"energy\", 0.0), default=0.0)\n",
    "    brightness = _safe_unit_float(meta.get(\"brightness\", 0.0), default=0.0)\n",
    "    mood = _safe_unit_float(meta.get(\"mood\", 0.0), default=0.0)\n",
    "    valence = _safe_unit_float(meta.get(\"valence\", 0.0), default=0.0)\n",
    "\n",
    "    audio_vibes = _audio_vibe_tokens(tempo, energy, brightness, mood, valence)\n",
    "    if audio_vibes:\n",
    "        vibe_tags = list(dict.fromkeys(vibe_tags + audio_vibes))\n",
    "\n",
    "    description = clean_description(str(meta.get(\"description\", \"\") or \"\"), max_chars=280)\n",
    "    instrumental = as_optional_bool(meta.get(\"instrumental\"))\n",
    "    if instrumental is True:\n",
    "        vocal_type = \"instrumental\"\n",
    "    elif instrumental is False:\n",
    "        vocal_type = \"non-instrumental\"\n",
    "    else:\n",
    "        vocal_type = \"unknown-vocals\"\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    # Context-heavy fields first.\n",
    "    if genres:\n",
    "        parts.append(f\"Genre profile: {' '.join(genres[:10])}\")\n",
    "    if tags:\n",
    "        parts.append(f\"Tag profile: {' '.join(tags[:16])}\")\n",
    "    if vibe_tags:\n",
    "        parts.append(f\"Vibe profile: {' '.join(vibe_tags[:16])}\")\n",
    "    if description:\n",
    "        parts.append(f\"Description: {description}\")\n",
    "\n",
    "    parts.append(\n",
    "        \"Audio feature scores: \"\n",
    "        f\"tempo {tempo:.3f}, energy {energy:.3f}, brightness {brightness:.3f}, mood {mood:.3f}, valence {valence:.3f}.\"\n",
    "    )\n",
    "    parts.append(\n",
    "        \"Audio feature buckets: \"\n",
    "        f\"tempo { _bucket(tempo) }, energy { _bucket(energy) }, brightness { _bucket(brightness) }, \"\n",
    "        f\"mood { _bucket(mood) }, valence { _bucket(valence) }.\"\n",
    "    )\n",
    "    parts.append(f\"Vocal type: {vocal_type}\")\n",
    "\n",
    "    # Keep identity with lower emphasis for exact-name queries.\n",
    "    parts.append(f\"Identity reference: artist {artist}. title {title}.\")\n",
    "\n",
    "    return \". \".join([p.strip() for p in parts if p and str(p).strip()])\n",
    "\n",
    "\n",
    "HAS_CUDA = torch.cuda.is_available()\n",
    "GPU_NAME = torch.cuda.get_device_name(0) if HAS_CUDA else \"CPU\"\n",
    "VRAM_GB = float(torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)) if HAS_CUDA else 0.0\n",
    "CPU_COUNT = os.cpu_count() or 4\n",
    "\n",
    "BATCH_SIZE = _suggest_embed_batch_size(VRAM_GB) if HAS_CUDA else 32\n",
    "EMBED_CHUNK_SIZE = max(BATCH_SIZE * 220, 22000)\n",
    "MAX_SEQ_LENGTH = 320\n",
    "EMBEDDINGS_PATH = Path(INDEX_DIR) / \"embeddings.npy\"\n",
    "EMBED_PROGRESS_PATH = Path(INDEX_DIR) / \"embeddings.progress.json\"\n",
    "\n",
    "if HAS_CUDA:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "model = embedding_handler.load_model()\n",
    "if model is not None:\n",
    "    try:\n",
    "        model.max_seq_length = int(MAX_SEQ_LENGTH)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if HAS_CUDA:\n",
    "        try:\n",
    "            model.half()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(f\"Device: {GPU_NAME}\")\n",
    "print(f\"VRAM: {VRAM_GB:.1f} GB | CPU cores: {CPU_COUNT}\")\n",
    "print(f\"Embedding batch size: {BATCH_SIZE}\")\n",
    "print(f\"Embedding chunk size: {EMBED_CHUNK_SIZE}\")\n",
    "print(f\"Tracks to encode: {len(ids)}\")\n",
    "if ids:\n",
    "    preview = _build_text_for_track(ids[0])\n",
    "    print(f\"Sample text: {preview[:240]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Encode in chunks and save with resume support\n",
    "if not ids:\n",
    "    raise RuntimeError(\"No track IDs found. Build metadata first.\")\n",
    "\n",
    "# Probe embedding dimension once.\n",
    "probe_vec = embedding_handler.encode([_build_text_for_track(ids[0])], batch_size=1)\n",
    "dim = int(probe_vec.shape[1])\n",
    "total = len(ids)\n",
    "\n",
    "resume_idx = 0\n",
    "if EMBEDDINGS_PATH.exists() and EMBED_PROGRESS_PATH.exists():\n",
    "    try:\n",
    "        progress = json.loads(EMBED_PROGRESS_PATH.read_text(encoding=\"utf-8\"))\n",
    "        if int(progress.get(\"count\", -1)) == total and int(progress.get(\"dim\", -1)) == dim:\n",
    "            resume_idx = int(progress.get(\"next_idx\", 0))\n",
    "            print(f\"Resuming embeddings from row {resume_idx}...\")\n",
    "    except Exception:\n",
    "        resume_idx = 0\n",
    "\n",
    "if EMBEDDINGS_PATH.exists() and resume_idx > 0:\n",
    "    embeddings_mm = np.load(EMBEDDINGS_PATH, mmap_mode=\"r+\")\n",
    "else:\n",
    "    embeddings_mm = np.lib.format.open_memmap(\n",
    "        str(EMBEDDINGS_PATH), mode=\"w+\", dtype=np.float32, shape=(total, dim)\n",
    "    )\n",
    "    resume_idx = 0\n",
    "\n",
    "for start in tqdm(range(resume_idx, total, EMBED_CHUNK_SIZE), desc=\"Encoding chunks\"):\n",
    "    end = min(start + EMBED_CHUNK_SIZE, total)\n",
    "    chunk_ids = ids[start:end]\n",
    "    chunk_texts = [_build_text_for_track(track_id) for track_id in chunk_ids]\n",
    "    chunk_embeddings = embedding_handler.encode(\n",
    "        chunk_texts,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=False,\n",
    "    ).astype(np.float32, copy=False)\n",
    "\n",
    "    embeddings_mm[start:end] = chunk_embeddings\n",
    "    embeddings_mm.flush()\n",
    "    EMBED_PROGRESS_PATH.write_text(\n",
    "        json.dumps({\"count\": total, \"dim\": dim, \"next_idx\": end}),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    del chunk_texts\n",
    "    del chunk_embeddings\n",
    "    gc.collect()\n",
    "    if HAS_CUDA:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "embeddings = np.load(EMBEDDINGS_PATH)\n",
    "if EMBED_PROGRESS_PATH.exists():\n",
    "    EMBED_PROGRESS_PATH.unlink()\n",
    "sync_path(EMBEDDINGS_PATH, \"indexes/embeddings.npy\")\n",
    "print(f\"Saved embeddings.npy shape={embeddings.shape} at {EMBEDDINGS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build FAISS Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "if embeddings.shape[0] != len(ids):\n",
    "    raise RuntimeError(f\"Count mismatch: embeddings={embeddings.shape[0]} vs id_map={len(ids)}\")\n",
    "\n",
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss_path = Path(INDEX_DIR) / \"faiss_index.bin\"\n",
    "faiss.write_index(index, str(faiss_path))\n",
    "sync_path(faiss_path, \"indexes/faiss_index.bin\")\n",
    "print(f\"FAISS index built and saved. ntotal={index.ntotal}, dim={index.d}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
