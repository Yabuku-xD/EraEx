{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EraEx Re-Embed + Re-Index (Colab, Local Copy Workflow)\n",
    "\n",
    "This notebook rebuilds only:\n",
    "- `embeddings.npy`\n",
    "- `faiss_index.bin`\n",
    "\n",
    "It is designed for **Google Colab** and **Google Drive**:\n",
    "1. Mount Drive\n",
    "2. Copy required project files to local Colab storage (`/content/...`) for faster reads/writes\n",
    "3. Rebuild embeddings + FAISS index locally\n",
    "4. Sync outputs back to Drive\n",
    "5. Optionally zip local outputs for easier download\n",
    "\n",
    "## Inputs required (already prepared)\n",
    "- `data/indexes/id_map.json`\n",
    "- `data/indexes/metadata.json`\n",
    "\n",
    "If you changed CSV enrichment/metadata fields, rebuild `id_map.json` + `metadata.json` first (in your full pipeline), then run this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install dependencies (Colab)\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install sentence-transformers faiss-cpu python-dotenv\n",
    "\n",
    "import os\n",
    "os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Mount Google Drive + configure paths\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Mount Drive\n",
    "DRIVE_MOUNT = '/content/drive'\n",
    "drive.mount(DRIVE_MOUNT)\n",
    "\n",
    "# Update this if your repo folder name is different in Drive.\n",
    "CANDIDATE_PROJECT_ROOTS = [\n",
    "    '/content/drive/MyDrive/Team4_CPSC-5830-01-Capstone-Project',\n",
    "    '/content/drive/MyDrive/EraEx',\n",
    "]\n",
    "\n",
    "DRIVE_PROJECT_ROOT = None\n",
    "for cand in CANDIDATE_PROJECT_ROOTS:\n",
    "    if Path(cand).exists():\n",
    "        DRIVE_PROJECT_ROOT = Path(cand)\n",
    "        break\n",
    "\n",
    "if DRIVE_PROJECT_ROOT is None:\n",
    "    raise FileNotFoundError(\n",
    "        'Could not auto-detect project root in Drive. Set DRIVE_PROJECT_ROOT manually.'\n",
    "    )\n",
    "\n",
    "LOCAL_ROOT = Path('/content/eraex_reembed_work')\n",
    "LOCAL_PROJECT_ROOT = LOCAL_ROOT / 'project'\n",
    "LOCAL_INDEX_DIR = LOCAL_PROJECT_ROOT / 'data' / 'indexes'\n",
    "DRIVE_INDEX_DIR = DRIVE_PROJECT_ROOT / 'data' / 'indexes'\n",
    "\n",
    "print('Drive project root:', DRIVE_PROJECT_ROOT)\n",
    "print('Local project root:', LOCAL_PROJECT_ROOT)\n",
    "\n",
    "# Clean local workspace to avoid stale files.\n",
    "if LOCAL_ROOT.exists():\n",
    "    shutil.rmtree(LOCAL_ROOT)\n",
    "LOCAL_INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy only what we need for re-embed/re-index (fast and lighter than copying whole repo).\n",
    "COPY_DIRS = ['src', 'config']\n",
    "COPY_FILES = [\n",
    "    'requirements.txt',\n",
    "    'data/indexes/id_map.json',\n",
    "    'data/indexes/metadata.json',\n",
    "]\n",
    "OPTIONAL_FILES = [\n",
    "    'data/indexes/embeddings.progress.json',  # resume support if present\n",
    "]\n",
    "\n",
    "for rel in COPY_DIRS:\n",
    "    src = DRIVE_PROJECT_ROOT / rel\n",
    "    dst = LOCAL_PROJECT_ROOT / rel\n",
    "    if src.exists():\n",
    "        shutil.copytree(src, dst)\n",
    "        print(f'Copied dir: {rel}')\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Missing required directory in Drive: {src}')\n",
    "\n",
    "for rel in COPY_FILES:\n",
    "    src = DRIVE_PROJECT_ROOT / rel\n",
    "    dst = LOCAL_PROJECT_ROOT / rel\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not src.exists():\n",
    "        raise FileNotFoundError(f'Missing required file in Drive: {src}')\n",
    "    shutil.copy2(src, dst)\n",
    "    print(f'Copied file: {rel}')\n",
    "\n",
    "for rel in OPTIONAL_FILES:\n",
    "    src = DRIVE_PROJECT_ROOT / rel\n",
    "    dst = LOCAL_PROJECT_ROOT / rel\n",
    "    if src.exists():\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f'Copied optional file: {rel}')\n",
    "\n",
    "# Make local project importable as `src.*`\n",
    "sys.path.insert(0, str(LOCAL_PROJECT_ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Imports (project + runtime libs)\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.core.text_embeddings import embedding_handler\n",
    "from src.core.media_metadata import build_track_embedding_text_context_first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load id_map + metadata from local copy\n",
    "INDEX_DIR = str(LOCAL_INDEX_DIR)\n",
    "ID_MAP_PATH = LOCAL_INDEX_DIR / 'id_map.json'\n",
    "METADATA_PATH = LOCAL_INDEX_DIR / 'metadata.json'\n",
    "\n",
    "with open(ID_MAP_PATH, 'r', encoding='utf-8') as f:\n",
    "    ids = json.load(f)\n",
    "with open(METADATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f'id_map count: {len(ids)}')\n",
    "print(f'metadata count: {len(metadata)}')\n",
    "if len(ids) != len(metadata):\n",
    "    print('WARNING: id_map and metadata counts differ. Embedding loop uses id_map order only.')\n",
    "\n",
    "# Optional quick sanity preview\n",
    "if ids:\n",
    "    first_id = str(ids[0])\n",
    "    print('Sample ID:', first_id)\n",
    "    print('Sample metadata keys:', sorted(list((metadata.get(first_id) or {}).keys()))[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Context-first embedding text builder (shared with runtime recommendation path)\n",
    "def _suggest_embed_batch_size(vram_gb):\n",
    "    if vram_gb >= 40:\n",
    "        return 256\n",
    "    if vram_gb >= 24:\n",
    "        return 160\n",
    "    if vram_gb >= 16:\n",
    "        return 96\n",
    "    if vram_gb >= 10:\n",
    "        return 64\n",
    "    return 32\n",
    "\n",
    "\n",
    "def _build_text_for_track(track_id):\n",
    "    meta = metadata.get(str(track_id), {})\n",
    "    return build_track_embedding_text_context_first(meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Embedding config + model setup\n",
    "HAS_CUDA = torch.cuda.is_available()\n",
    "GPU_NAME = torch.cuda.get_device_name(0) if HAS_CUDA else 'CPU'\n",
    "VRAM_GB = float(torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)) if HAS_CUDA else 0.0\n",
    "CPU_COUNT = os.cpu_count() or 4\n",
    "\n",
    "BATCH_SIZE = _suggest_embed_batch_size(VRAM_GB) if HAS_CUDA else 32\n",
    "EMBED_CHUNK_SIZE = max(BATCH_SIZE * 220, 22000)\n",
    "MAX_SEQ_LENGTH = 320\n",
    "EMBEDDINGS_PATH = LOCAL_INDEX_DIR / 'embeddings.npy'\n",
    "EMBED_PROGRESS_PATH = LOCAL_INDEX_DIR / 'embeddings.progress.json'\n",
    "\n",
    "if HAS_CUDA:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "model = embedding_handler.load_model()\n",
    "if model is not None:\n",
    "    try:\n",
    "        model.max_seq_length = int(MAX_SEQ_LENGTH)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if HAS_CUDA:\n",
    "        try:\n",
    "            model.half()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(f'Device: {GPU_NAME}')\n",
    "print(f'VRAM: {VRAM_GB:.1f} GB | CPU cores: {CPU_COUNT}')\n",
    "print(f'Embedding batch size: {BATCH_SIZE}')\n",
    "print(f'Embedding chunk size: {EMBED_CHUNK_SIZE}')\n",
    "print(f'Tracks to encode: {len(ids)}')\n",
    "if ids:\n",
    "    preview = _build_text_for_track(ids[0])\n",
    "    print(f'Sample text: {preview[:280]}...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Generate / resume embeddings.npy (local Colab disk)\n",
    "def _load_resume_index(progress_path):\n",
    "    if not progress_path.exists():\n",
    "        return 0\n",
    "    try:\n",
    "        with open(progress_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        idx = int(data.get('next_index', 0))\n",
    "        return max(0, min(idx, len(ids)))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def _save_resume_index(progress_path, next_index):\n",
    "    payload = {\n",
    "        'next_index': int(next_index),\n",
    "        'total': int(len(ids)),\n",
    "        'updated_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    }\n",
    "    with open(progress_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Probe dimension\n",
    "probe_vec = embedding_handler.encode([_build_text_for_track(ids[0])], batch_size=1)\n",
    "d = int(probe_vec.shape[1])\n",
    "print('Embedding dim:', d)\n",
    "del probe_vec\n",
    "gc.collect()\n",
    "if HAS_CUDA:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "resume_idx = _load_resume_index(EMBED_PROGRESS_PATH)\n",
    "if EMBEDDINGS_PATH.exists():\n",
    "    embeddings_mm = np.load(EMBEDDINGS_PATH, mmap_mode='r+')\n",
    "    if embeddings_mm.shape != (len(ids), d):\n",
    "        print('Existing embeddings shape mismatch. Recreating memmap...')\n",
    "        del embeddings_mm\n",
    "        EMBEDDINGS_PATH.unlink(missing_ok=True)\n",
    "        embeddings_mm = np.lib.format.open_memmap(\n",
    "            EMBEDDINGS_PATH, mode='w+', dtype=np.float32, shape=(len(ids), d)\n",
    "        )\n",
    "        resume_idx = 0\n",
    "else:\n",
    "    embeddings_mm = np.lib.format.open_memmap(\n",
    "        EMBEDDINGS_PATH, mode='w+', dtype=np.float32, shape=(len(ids), d)\n",
    "    )\n",
    "\n",
    "if resume_idx:\n",
    "    print(f'Resuming embeddings from row {resume_idx}...')\n",
    "\n",
    "for start in tqdm(range(resume_idx, len(ids), EMBED_CHUNK_SIZE), desc='Embedding chunks'):\n",
    "    end = min(start + EMBED_CHUNK_SIZE, len(ids))\n",
    "    chunk_ids = ids[start:end]\n",
    "    chunk_texts = [_build_text_for_track(track_id) for track_id in chunk_ids]\n",
    "    chunk_embeddings = embedding_handler.encode(\n",
    "        chunk_texts,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=False,\n",
    "        normalize_embeddings=True,\n",
    "    ).astype(np.float32)\n",
    "    embeddings_mm[start:end] = chunk_embeddings\n",
    "    embeddings_mm.flush()\n",
    "    _save_resume_index(EMBED_PROGRESS_PATH, end)\n",
    "\n",
    "    del chunk_texts, chunk_embeddings\n",
    "    gc.collect()\n",
    "    if HAS_CUDA:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Final load check\n",
    "embeddings = np.load(EMBEDDINGS_PATH)\n",
    "print(f'Saved embeddings.npy shape={embeddings.shape} at {EMBEDDINGS_PATH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Build FAISS index (inner product) from embeddings.npy\n",
    "if len(ids) == 0:\n",
    "    raise RuntimeError('No ids found in id_map.json')\n",
    "if embeddings.shape[0] != len(ids):\n",
    "    raise RuntimeError(f'Count mismatch: embeddings={embeddings.shape[0]} vs id_map={len(ids)}')\n",
    "\n",
    "d = int(embeddings.shape[1])\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss_path = LOCAL_INDEX_DIR / 'faiss_index.bin'\n",
    "faiss.write_index(index, str(faiss_path))\n",
    "print(f'Saved faiss_index.bin ntotal={index.ntotal}, dim={d} at {faiss_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Sync outputs back to Drive + prepare local zip for download\n",
    "from google.colab import files\n",
    "\n",
    "SYNC_OUTPUTS = [\n",
    "    'data/indexes/embeddings.npy',\n",
    "    'data/indexes/embeddings.progress.json',\n",
    "    'data/indexes/faiss_index.bin',\n",
    "]\n",
    "\n",
    "for rel in SYNC_OUTPUTS:\n",
    "    src = LOCAL_PROJECT_ROOT / rel\n",
    "    dst = DRIVE_PROJECT_ROOT / rel\n",
    "    if src.exists():\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f'Synced -> Drive: {rel}')\n",
    "\n",
    "# Create a local zip containing only the regenerated index artifacts (easy Colab download)\n",
    "zip_base = '/content/eraex_reembed_outputs'\n",
    "zip_path = shutil.make_archive(zip_base, 'zip', root_dir=LOCAL_PROJECT_ROOT, base_dir='data/indexes')\n",
    "print('Local zip ready:', zip_path)\n",
    "print('You can download it with: files.download(zip_path)')\n",
    "\n",
    "# Uncomment if you want immediate browser download (large files may take a while)\n",
    "# files.download(zip_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- This notebook **does not rebuild** `id_map.json` or `metadata.json`.\n",
    "- It reuses your existing metadata and only regenerates embeddings + FAISS index.\n",
    "- If you change embedding text logic again, rerun **Step 7** and **Step 8**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  },
  "colab": {
   "name": "reembed_reindex_colab.ipynb",
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}