{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EraEx: Combined CSV Ingestion (Colab)\n",
                "\n",
                "This notebook reads **BOTH** CSV files and converts them to Parquet:\n",
                "1. `dataset.csv` (existing data)\n",
                "2. `ndjson_converted.csv` (from notebook 01)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import polars as pl\n",
                "from datetime import date\n",
                "\n",
                "PROJECT_DIR = Path('/content/drive/MyDrive/EraEx')\n",
                "RAW_DIR = PROJECT_DIR / 'data' / 'raw'\n",
                "PROCESSED_DIR = PROJECT_DIR / 'data' / 'processed'\n",
                "\n",
                "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "YEAR_RANGE = range(2012, 2019)\n",
                "\n",
                "CSV_FILES = [\n",
                "    RAW_DIR / 'dataset.csv',\n",
                "    RAW_DIR / 'ndjson_converted.csv'\n",
                "]\n",
                "\n",
                "print('CSV files to process:')\n",
                "for f in CSV_FILES:\n",
                "    if f.exists():\n",
                "        print(f'  ✓ {f.name} ({f.stat().st_size / 1e9:.2f} GB)')\n",
                "    else:\n",
                "        print(f'  ✗ {f.name} (NOT FOUND)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "COLUMN_MAPPING = {\n",
                "    'id': 'track_id',\n",
                "    'soundcloud_id': 'track_id',\n",
                "    'user': 'artist',\n",
                "    'username': 'artist',\n",
                "    'tag_list': 'tags',\n",
                "    'plays': 'playback_count',\n",
                "    'url': 'permalink_url',\n",
                "    'date': 'created_at',\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ingest_csv(csv_path: Path, batch_counter: int = 0):\n",
                "    if not csv_path.exists():\n",
                "        print(f'Skipping {csv_path.name} (not found)')\n",
                "        return batch_counter, {}\n",
                "    \n",
                "    print(f'\\nProcessing: {csv_path.name}')\n",
                "    \n",
                "    stats = {year: 0 for year in YEAR_RANGE}\n",
                "    \n",
                "    reader = pl.read_csv_batched(\n",
                "        csv_path,\n",
                "        batch_size=500000,\n",
                "        ignore_errors=True,\n",
                "        truncate_ragged_lines=True,\n",
                "        infer_schema_length=10000\n",
                "    )\n",
                "    \n",
                "    while True:\n",
                "        batches = reader.next_batches(1)\n",
                "        if not batches:\n",
                "            break\n",
                "        \n",
                "        df = batches[0]\n",
                "        \n",
                "        rename_map = {}\n",
                "        for old_col in df.columns:\n",
                "            old_lower = old_col.lower().strip()\n",
                "            if old_lower in COLUMN_MAPPING:\n",
                "                rename_map[old_col] = COLUMN_MAPPING[old_lower]\n",
                "        if rename_map:\n",
                "            df = df.rename(rename_map)\n",
                "        \n",
                "        if 'track_id' not in df.columns:\n",
                "            if 'id' in df.columns:\n",
                "                df = df.rename({'id': 'track_id'})\n",
                "        \n",
                "        if 'track_id' in df.columns:\n",
                "            df = df.with_columns([pl.col('track_id').cast(pl.Utf8)])\n",
                "        \n",
                "        if 'year' not in df.columns and 'created_at' in df.columns:\n",
                "            df = df.with_columns([\n",
                "                pl.col('created_at').cast(pl.Utf8).str.slice(0, 4).cast(pl.Int32, strict=False).alias('year')\n",
                "            ])\n",
                "        \n",
                "        df = df.with_columns([pl.lit(date.today()).alias('ingest_date')])\n",
                "        \n",
                "        for year in YEAR_RANGE:\n",
                "            year_df = df.filter(pl.col('year') == year)\n",
                "            if year_df.height == 0:\n",
                "                continue\n",
                "            \n",
                "            stats[year] += year_df.height\n",
                "            \n",
                "            year_dir = PROCESSED_DIR / f'year={year}'\n",
                "            year_dir.mkdir(parents=True, exist_ok=True)\n",
                "            \n",
                "            batch_counter += 1\n",
                "            out_path = year_dir / f'batch_{batch_counter:05d}.parquet'\n",
                "            \n",
                "            year_df = year_df.with_columns([pl.col('ingest_date').cast(pl.Date)])\n",
                "            year_df.write_parquet(out_path)\n",
                "        \n",
                "        total_so_far = sum(stats.values())\n",
                "        if total_so_far % 500000 == 0:\n",
                "            print(f'  Processed: {total_so_far:,} rows')\n",
                "    \n",
                "    return batch_counter, stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_counter = 0\n",
                "all_stats = {}\n",
                "\n",
                "for csv_file in CSV_FILES:\n",
                "    batch_counter, stats = ingest_csv(csv_file, batch_counter)\n",
                "    all_stats[csv_file.name] = stats\n",
                "    print(f\"  Total: {sum(stats.values()):,} rows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('\\n' + '=' * 50)\n",
                "print('INGEST COMPLETE')\n",
                "print('=' * 50)\n",
                "\n",
                "grand_total = 0\n",
                "for year_dir in sorted(PROCESSED_DIR.glob('year=*')):\n",
                "    parquet_files = list(year_dir.glob('*.parquet'))\n",
                "    total_rows = sum(pl.scan_parquet(f).select(pl.count()).collect().item() for f in parquet_files)\n",
                "    grand_total += total_rows\n",
                "    print(f'{year_dir.name}: {total_rows:,} rows in {len(parquet_files)} files')\n",
                "\n",
                "print(f'\\nGrand Total: {grand_total:,} rows')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
